{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d2f04a7",
   "metadata": {},
   "source": [
    "# <Semi-supervised learning tutorial 3 - consistency regularization>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31dcf363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'LG_semi_supervised_learning_day17' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/KU-DIC/LG_semi_supervised_learning_day17.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a37bbef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import easydict\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from LG_semi_supervised_learning_day17.augmentation import RandAugmentCIFAR\n",
    "from LG_semi_supervised_learning_day17.models import WideResNet, ModelEMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1953bbe",
   "metadata": {},
   "source": [
    "### 하이퍼파라미터세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89cab206",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "    \"seed\" : 0,\n",
    "    \"gpu\": 0,\n",
    "    \"start_step\" : 0,\n",
    "    \"total_steps\" : 2000, # 30000\n",
    "    \"eval_step\" : 20, # 100\n",
    "    \"lambda_u\" : 10,\n",
    "    \n",
    "    # for data\n",
    "    \"data_path\" : \"/content/LG_semi_supervised_learning_day17/data\",\n",
    "    \"num_data\" : 10000, # 50000\n",
    "    \"num_labeled\" : 1000,# 5000 \n",
    "    \"num_classes\" : 10, # number of classes\n",
    "    \"resize\" : 32, # resize image\n",
    "    \"batch_size\" : 64,\n",
    "    \"mu\" : 1, # coefficient of unlabeled batch size,\n",
    "    \n",
    "    # for WideResNet model\n",
    "    \"depth\" : 10,\n",
    "    \"widen_factor\" : 1,\n",
    "    \"teacher_dropout\" : 0, # dropout on last dense layer of teacher model\n",
    "    \"student_dropout\" : 0, # dropout on last dense layer of student model\n",
    "    \n",
    "    # for optimizing\n",
    "    \"lr\" : 0.01, # train learning rate of model\n",
    "    \"momentum\" : 0.9, # SGD Momentum\n",
    "    \"nesterov\" : True, # use nesterov\n",
    "    \"weight_decay\" : 0.01, # train weight decay\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a79296c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.device = torch.device('cuda', args.gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3850e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a6e5b2",
   "metadata": {},
   "source": [
    "### 데이터셋 & 데이터로더 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "888cbb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "base_dataset = datasets.CIFAR10(args.data_path, train=True, download=True)\n",
    "test_dataset = datasets.CIFAR10(args.data_path, train=False, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94ad4476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_u_split(args, labels):\n",
    "    \n",
    "    label_per_class = args.num_labeled // args.num_classes\n",
    "    num_unlabel_data = ((args.num_data // args.num_classes) - label_per_class) * args.num_classes\n",
    "    # 학습 시간을 줄이기 위해서 데이터 개수를 줄이기 위해서 추가\n",
    "    \n",
    "    print(f'클래스별 labeled data 개수 : {label_per_class}')\n",
    "    print(f'Labeled data 개수 : {label_per_class * args.num_classes}')\n",
    "    print(f'Unlabeled data 개수 : {num_unlabel_data}')\n",
    "    \n",
    "    labels = np.array(labels)\n",
    "    labeled_idx = []\n",
    "    \n",
    "    unlabeled_idx = np.array(range(len(labels))) \n",
    "    for i in range(args.num_classes):\n",
    "        idx = np.where(labels == i)[0]\n",
    "        idx = np.random.choice(idx, label_per_class, False)\n",
    "        labeled_idx.extend(idx)\n",
    "    labeled_idx = np.array(labeled_idx)\n",
    "    np.random.shuffle(labeled_idx)\n",
    "    \n",
    "    unlabeled_idx = np.array([i for i in unlabeled_idx if i not in labeled_idx])\n",
    "    np.random.shuffle(unlabeled_idx)\n",
    "    unlabeled_idx = unlabeled_idx[:num_unlabel_data]\n",
    "    \n",
    "    return labeled_idx, unlabeled_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c4c6371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스별 labeled data 개수 : 100\n",
      "Labeled data 개수 : 1000\n",
      "Unlabeled data 개수 : 9000\n"
     ]
    }
   ],
   "source": [
    "labeled_idxs, unlabeled_idxs = l_u_split(args, base_dataset.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59ec4454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화에 사용될 평균, 표준편차\n",
    "cifar10_mean = (0.491400, 0.482158, 0.4465231)\n",
    "cifar10_std = (0.247032, 0.243485, 0.2615877)\n",
    "\n",
    "# # 데이터셋을 위한 커스터마이징된 데이터변환 클래스 만들기\n",
    "class CustomTransform(object):\n",
    "    # class 초기화\n",
    "    def __init__(self, args, n, m, mean, std):\n",
    "        self.n, self.m = n, m\n",
    "        \n",
    "        self.ori = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(size=args.resize,\n",
    "                                  padding=int(args.resize * 0.125),\n",
    "                                  fill=128,\n",
    "                                  padding_mode='constant')])\n",
    "        \n",
    "        self.aug = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(size=args.resize,\n",
    "                                  padding=int(args.resize * 0.125),\n",
    "                                  fill=128,\n",
    "                                  padding_mode='constant'),\n",
    "            RandAugmentCIFAR(n=n, m=m)])\n",
    "        \n",
    "        self.normalize = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=std)])\n",
    "        \n",
    "    # class가 사용될 때\n",
    "    def __call__(self, x):\n",
    "        ori = self.ori(x)\n",
    "        aug = self.aug(x)\n",
    "        return self.normalize(ori), self.normalize(aug)\n",
    "    \n",
    "# Labeled & Unlabeled 데이터셋을 위한 데이터변환 사전에 정의\n",
    "transform_labeled = CustomTransform(args, n=5, m=20, mean=cifar10_mean, std=cifar10_std)\n",
    "transform_unlabeled = CustomTransform(args, n=5, m=20, mean=cifar10_mean, std=cifar10_std)\n",
    "\n",
    "# Test 데이터셋을 위한 데이터변환 사전에 정의\n",
    "transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=cifar10_mean, std=cifar10_std)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bd8ba00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCIFAR10SSL(datasets.CIFAR10):\n",
    "    def __init__(self, root, indexs, train=True,\n",
    "                 transform=None, target_transform=None, download=False):\n",
    "        super().__init__(root, train=train,\n",
    "                         transform=transform,\n",
    "                         target_transform=target_transform,\n",
    "                         download=download)\n",
    "        if indexs is not None:\n",
    "            self.data = self.data[indexs]\n",
    "            self.targets = np.array(self.targets)[indexs]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed828c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_dataset = CustomCIFAR10SSL(args.data_path, \n",
    "                                   labeled_idxs, \n",
    "                                   train=True, \n",
    "                                   transform=transform_labeled)\n",
    "unlabeled_dataset = CustomCIFAR10SSL(args.data_path, \n",
    "                                     unlabeled_idxs, \n",
    "                                     train=True, \n",
    "                                     transform=transform_unlabeled)\n",
    "test_dataset = datasets.CIFAR10(args.data_path, train=False, transform=transform_test, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efbf8926",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_loader = DataLoader(labeled_dataset, sampler=RandomSampler(labeled_dataset),\n",
    "                            batch_size=args.batch_size, drop_last=True)\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, sampler=RandomSampler(unlabeled_dataset),\n",
    "                              batch_size=args.batch_size * args.mu, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767b68f5",
   "metadata": {},
   "source": [
    "### Labeled & unlabeled 데이터셋을 사용한 semi-supervised learning\n",
    "#### Consistency regularization 예시1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb2ae980",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WideResNet(num_classes=args.num_classes,\n",
    "                   depth=args.depth,\n",
    "                   widen_factor=args.widen_factor,\n",
    "                   dropout=0,\n",
    "                   dense_dropout=args.teacher_dropout)\n",
    "model.to(args.device)\n",
    "optimizer = optim.SGD(model.parameters(),\n",
    "                      lr=args.lr,\n",
    "                      momentum=args.momentum,\n",
    "                      nesterov=args.nesterov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d917910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_consistency_regularization(args, model, optimizer):\n",
    "    since = time.time()\n",
    "    for step in range(args.start_step, args.total_steps):\n",
    "        if step % args.eval_step == 0:\n",
    "            if step != 0:\n",
    "                print('{} Step - loss: {:.4f} '.format(step,np.mean(losses)))\n",
    "                print('loss for label: {:.4f}, loss for unlabel: {:.4f} '.format(np.mean(losses_l),np.mean(losses_u)))\n",
    "        \n",
    "            losses = []\n",
    "            losses_u = []\n",
    "            losses_l = []\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        try:\n",
    "            (images_lw, images_ls), targets = labeled_iter.next()\n",
    "        except:\n",
    "            labeled_iter = iter(labeled_loader)\n",
    "            (images_lw, images_ls), targets = labeled_iter.next()\n",
    "\n",
    "        try:\n",
    "            (images_uw, images_us), _ = unlabeled_iter.next()\n",
    "        except:\n",
    "            unlabeled_iter = iter(unlabeled_loader)\n",
    "            (images_uw, images_us), _ = unlabeled_iter.next()\n",
    "\n",
    "        images_lw = images_lw.to(args.device)\n",
    "        images_ls = images_ls.to(args.device)\n",
    "        images_uw = images_uw.to(args.device)\n",
    "        images_us = images_us.to(args.device)\n",
    "        targets = targets.to(args.device, dtype=torch.long)\n",
    "\n",
    "        # parameter gradients를 0으로 설정\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward model\n",
    "        batch_size = images_ls.shape[0]\n",
    "        images = torch.cat((images_ls, images_lw, images_uw, images_us))\n",
    "        logits = model(images)\n",
    "        logits_ls = logits[:batch_size]\n",
    "        logits_lw = logits[batch_size:2*batch_size]\n",
    "        logits_uw, logits_us = logits[2*batch_size:].chunk(2)\n",
    "        del logits\n",
    "        \n",
    "        # loss for for labeled data\n",
    "        ls_pred_prob = torch.softmax(logits_ls, dim=-1)\n",
    "        lw_pred_prob = torch.softmax(logits_lw, dim=-1)\n",
    "        \n",
    "        loss_l_u = F.mse_loss(ls_pred_prob,lw_pred_prob, reduction='mean')\n",
    "        loss_l_s = F.cross_entropy(logits_lw, targets, reduction='mean')\n",
    "\n",
    "        # loss for for unlabeled data\n",
    "        us_pred_prob = torch.softmax(logits_us, dim=-1)\n",
    "        uw_pred_prob = torch.softmax(logits_uw, dim=-1)\n",
    "        loss_u_u = F.mse_loss(us_pred_prob,uw_pred_prob, reduction='mean')\n",
    "        \n",
    "        loss_u = (loss_l_u + loss_u_u)/2\n",
    "        \n",
    "        # total loss\n",
    "        loss = loss_l_s + args.lambda_u * loss_u\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        losses_u.append(loss_u.item())\n",
    "        losses_l.append(loss_l_s.item())\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f417d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 Step - loss: 2.2818 \n",
      "loss for label: 2.2795, loss for unlabel: 0.0023 \n",
      "40 Step - loss: 2.0494 \n",
      "loss for label: 2.0445, loss for unlabel: 0.0049 \n",
      "60 Step - loss: 1.9698 \n",
      "loss for label: 1.9622, loss for unlabel: 0.0075 \n",
      "80 Step - loss: 1.9019 \n",
      "loss for label: 1.8928, loss for unlabel: 0.0091 \n",
      "100 Step - loss: 1.8722 \n",
      "loss for label: 1.8621, loss for unlabel: 0.0101 \n",
      "120 Step - loss: 1.8318 \n",
      "loss for label: 1.8214, loss for unlabel: 0.0104 \n",
      "140 Step - loss: 1.8077 \n",
      "loss for label: 1.7960, loss for unlabel: 0.0117 \n",
      "160 Step - loss: 1.7997 \n",
      "loss for label: 1.7875, loss for unlabel: 0.0122 \n",
      "180 Step - loss: 1.7832 \n",
      "loss for label: 1.7710, loss for unlabel: 0.0122 \n",
      "200 Step - loss: 1.7193 \n",
      "loss for label: 1.7061, loss for unlabel: 0.0131 \n",
      "220 Step - loss: 1.7301 \n",
      "loss for label: 1.7157, loss for unlabel: 0.0144 \n",
      "240 Step - loss: 1.7096 \n",
      "loss for label: 1.6949, loss for unlabel: 0.0147 \n",
      "260 Step - loss: 1.6651 \n",
      "loss for label: 1.6489, loss for unlabel: 0.0162 \n",
      "280 Step - loss: 1.6655 \n",
      "loss for label: 1.6485, loss for unlabel: 0.0170 \n",
      "300 Step - loss: 1.6472 \n",
      "loss for label: 1.6300, loss for unlabel: 0.0171 \n",
      "320 Step - loss: 1.6346 \n",
      "loss for label: 1.6168, loss for unlabel: 0.0178 \n",
      "340 Step - loss: 1.6029 \n",
      "loss for label: 1.5846, loss for unlabel: 0.0183 \n",
      "360 Step - loss: 1.5906 \n",
      "loss for label: 1.5716, loss for unlabel: 0.0190 \n",
      "380 Step - loss: 1.5816 \n",
      "loss for label: 1.5619, loss for unlabel: 0.0197 \n",
      "400 Step - loss: 1.5529 \n",
      "loss for label: 1.5334, loss for unlabel: 0.0195 \n",
      "420 Step - loss: 1.5285 \n",
      "loss for label: 1.5087, loss for unlabel: 0.0197 \n",
      "440 Step - loss: 1.5187 \n",
      "loss for label: 1.4974, loss for unlabel: 0.0213 \n",
      "460 Step - loss: 1.5097 \n",
      "loss for label: 1.4885, loss for unlabel: 0.0212 \n",
      "480 Step - loss: 1.5092 \n",
      "loss for label: 1.4866, loss for unlabel: 0.0226 \n",
      "500 Step - loss: 1.4878 \n",
      "loss for label: 1.4651, loss for unlabel: 0.0227 \n",
      "520 Step - loss: 1.4310 \n",
      "loss for label: 1.4082, loss for unlabel: 0.0228 \n",
      "540 Step - loss: 1.4809 \n",
      "loss for label: 1.4577, loss for unlabel: 0.0232 \n",
      "560 Step - loss: 1.4294 \n",
      "loss for label: 1.4060, loss for unlabel: 0.0234 \n",
      "580 Step - loss: 1.4182 \n",
      "loss for label: 1.3942, loss for unlabel: 0.0239 \n",
      "600 Step - loss: 1.4304 \n",
      "loss for label: 1.4064, loss for unlabel: 0.0240 \n",
      "620 Step - loss: 1.4158 \n",
      "loss for label: 1.3911, loss for unlabel: 0.0247 \n",
      "640 Step - loss: 1.3941 \n",
      "loss for label: 1.3691, loss for unlabel: 0.0250 \n",
      "660 Step - loss: 1.3828 \n",
      "loss for label: 1.3577, loss for unlabel: 0.0251 \n",
      "680 Step - loss: 1.3435 \n",
      "loss for label: 1.3184, loss for unlabel: 0.0251 \n",
      "700 Step - loss: 1.3506 \n",
      "loss for label: 1.3246, loss for unlabel: 0.0260 \n",
      "720 Step - loss: 1.3386 \n",
      "loss for label: 1.3125, loss for unlabel: 0.0261 \n",
      "740 Step - loss: 1.3220 \n",
      "loss for label: 1.2943, loss for unlabel: 0.0277 \n",
      "760 Step - loss: 1.2892 \n",
      "loss for label: 1.2620, loss for unlabel: 0.0272 \n",
      "780 Step - loss: 1.3075 \n",
      "loss for label: 1.2789, loss for unlabel: 0.0287 \n",
      "800 Step - loss: 1.2900 \n",
      "loss for label: 1.2621, loss for unlabel: 0.0279 \n",
      "820 Step - loss: 1.3077 \n",
      "loss for label: 1.2801, loss for unlabel: 0.0275 \n",
      "840 Step - loss: 1.2610 \n",
      "loss for label: 1.2317, loss for unlabel: 0.0293 \n",
      "860 Step - loss: 1.2837 \n",
      "loss for label: 1.2538, loss for unlabel: 0.0299 \n",
      "880 Step - loss: 1.2074 \n",
      "loss for label: 1.1787, loss for unlabel: 0.0287 \n",
      "900 Step - loss: 1.2580 \n",
      "loss for label: 1.2280, loss for unlabel: 0.0300 \n",
      "920 Step - loss: 1.2129 \n",
      "loss for label: 1.1834, loss for unlabel: 0.0295 \n",
      "940 Step - loss: 1.1948 \n",
      "loss for label: 1.1651, loss for unlabel: 0.0297 \n",
      "960 Step - loss: 1.2256 \n",
      "loss for label: 1.1939, loss for unlabel: 0.0317 \n",
      "980 Step - loss: 1.1906 \n",
      "loss for label: 1.1593, loss for unlabel: 0.0313 \n",
      "1000 Step - loss: 1.1858 \n",
      "loss for label: 1.1545, loss for unlabel: 0.0312 \n",
      "1020 Step - loss: 1.1909 \n",
      "loss for label: 1.1598, loss for unlabel: 0.0312 \n",
      "1040 Step - loss: 1.1565 \n",
      "loss for label: 1.1242, loss for unlabel: 0.0323 \n",
      "1060 Step - loss: 1.1414 \n",
      "loss for label: 1.1090, loss for unlabel: 0.0324 \n",
      "1080 Step - loss: 1.1438 \n",
      "loss for label: 1.1106, loss for unlabel: 0.0332 \n",
      "1100 Step - loss: 1.1301 \n",
      "loss for label: 1.0961, loss for unlabel: 0.0339 \n",
      "1120 Step - loss: 1.1185 \n",
      "loss for label: 1.0861, loss for unlabel: 0.0324 \n",
      "1140 Step - loss: 1.1214 \n",
      "loss for label: 1.0876, loss for unlabel: 0.0337 \n",
      "1160 Step - loss: 1.1084 \n",
      "loss for label: 1.0736, loss for unlabel: 0.0348 \n",
      "1180 Step - loss: 1.1010 \n",
      "loss for label: 1.0667, loss for unlabel: 0.0343 \n",
      "1200 Step - loss: 1.1087 \n",
      "loss for label: 1.0729, loss for unlabel: 0.0358 \n",
      "1220 Step - loss: 1.0589 \n",
      "loss for label: 1.0228, loss for unlabel: 0.0361 \n",
      "1240 Step - loss: 1.1130 \n",
      "loss for label: 1.0783, loss for unlabel: 0.0348 \n",
      "1260 Step - loss: 1.0911 \n",
      "loss for label: 1.0563, loss for unlabel: 0.0348 \n",
      "1280 Step - loss: 1.0213 \n",
      "loss for label: 0.9853, loss for unlabel: 0.0360 \n",
      "1300 Step - loss: 1.0419 \n",
      "loss for label: 1.0042, loss for unlabel: 0.0377 \n",
      "1320 Step - loss: 1.0357 \n",
      "loss for label: 0.9997, loss for unlabel: 0.0360 \n",
      "1340 Step - loss: 1.0340 \n",
      "loss for label: 0.9972, loss for unlabel: 0.0368 \n",
      "1360 Step - loss: 1.0001 \n",
      "loss for label: 0.9620, loss for unlabel: 0.0381 \n",
      "1380 Step - loss: 1.0225 \n",
      "loss for label: 0.9843, loss for unlabel: 0.0382 \n",
      "1400 Step - loss: 0.9690 \n",
      "loss for label: 0.9313, loss for unlabel: 0.0376 \n",
      "1420 Step - loss: 0.9946 \n",
      "loss for label: 0.9565, loss for unlabel: 0.0381 \n",
      "1440 Step - loss: 0.9943 \n",
      "loss for label: 0.9562, loss for unlabel: 0.0381 \n",
      "1460 Step - loss: 0.9721 \n",
      "loss for label: 0.9325, loss for unlabel: 0.0396 \n",
      "1480 Step - loss: 0.9574 \n",
      "loss for label: 0.9183, loss for unlabel: 0.0391 \n",
      "1500 Step - loss: 0.9394 \n",
      "loss for label: 0.8983, loss for unlabel: 0.0411 \n",
      "1520 Step - loss: 0.9164 \n",
      "loss for label: 0.8760, loss for unlabel: 0.0404 \n",
      "1540 Step - loss: 0.9674 \n",
      "loss for label: 0.9283, loss for unlabel: 0.0391 \n",
      "1560 Step - loss: 0.9403 \n",
      "loss for label: 0.8993, loss for unlabel: 0.0410 \n",
      "1580 Step - loss: 0.9321 \n",
      "loss for label: 0.8899, loss for unlabel: 0.0422 \n",
      "1600 Step - loss: 0.8989 \n",
      "loss for label: 0.8553, loss for unlabel: 0.0436 \n",
      "1620 Step - loss: 0.9343 \n",
      "loss for label: 0.8911, loss for unlabel: 0.0432 \n",
      "1640 Step - loss: 0.9106 \n",
      "loss for label: 0.8675, loss for unlabel: 0.0431 \n",
      "1660 Step - loss: 0.9171 \n",
      "loss for label: 0.8738, loss for unlabel: 0.0432 \n",
      "1680 Step - loss: 0.8720 \n",
      "loss for label: 0.8288, loss for unlabel: 0.0433 \n",
      "1700 Step - loss: 0.8616 \n",
      "loss for label: 0.8182, loss for unlabel: 0.0434 \n",
      "1720 Step - loss: 0.9018 \n",
      "loss for label: 0.8587, loss for unlabel: 0.0430 \n",
      "1740 Step - loss: 0.8640 \n",
      "loss for label: 0.8193, loss for unlabel: 0.0447 \n",
      "1760 Step - loss: 0.8761 \n",
      "loss for label: 0.8310, loss for unlabel: 0.0451 \n",
      "1780 Step - loss: 0.8382 \n",
      "loss for label: 0.7930, loss for unlabel: 0.0452 \n",
      "1800 Step - loss: 0.8395 \n",
      "loss for label: 0.7933, loss for unlabel: 0.0461 \n",
      "1820 Step - loss: 0.8279 \n",
      "loss for label: 0.7831, loss for unlabel: 0.0449 \n",
      "1840 Step - loss: 0.8155 \n",
      "loss for label: 0.7680, loss for unlabel: 0.0474 \n",
      "1860 Step - loss: 0.8293 \n",
      "loss for label: 0.7828, loss for unlabel: 0.0465 \n",
      "1880 Step - loss: 0.8081 \n",
      "loss for label: 0.7611, loss for unlabel: 0.0470 \n",
      "1900 Step - loss: 0.7548 \n",
      "loss for label: 0.7064, loss for unlabel: 0.0484 \n",
      "1920 Step - loss: 0.8117 \n",
      "loss for label: 0.7639, loss for unlabel: 0.0478 \n",
      "1940 Step - loss: 0.7750 \n",
      "loss for label: 0.7276, loss for unlabel: 0.0474 \n",
      "1960 Step - loss: 0.7781 \n",
      "loss for label: 0.7287, loss for unlabel: 0.0494 \n",
      "1980 Step - loss: 0.7854 \n",
      "loss for label: 0.7366, loss for unlabel: 0.0488 \n",
      "Training complete in 4m 21s\n"
     ]
    }
   ],
   "source": [
    "train_consistency_regularization(args, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9a82a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        corrects = 0\n",
    "        total = 0\n",
    "        for inputs, targets in loader:\n",
    "            inputs = inputs.to(args.device)\n",
    "            targets = targets.to(args.device, dtype=torch.long)\n",
    "\n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # output 중 최대값의 위치에 해당하는 class로 예측 수행\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            # batch별 정답 개수를 축적함\n",
    "            corrects += torch.sum(preds == targets.data)\n",
    "            total += targets.size(0)\n",
    "\n",
    "    test_acc = corrects.double() / total\n",
    "    print('Testing Acc: {:.4f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f54c8aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Acc: 0.3294\n"
     ]
    }
   ],
   "source": [
    "test(args, model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
