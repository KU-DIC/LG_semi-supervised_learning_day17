{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d2f04a7",
   "metadata": {},
   "source": [
    "# <Semi-supervised learning tutorial 3 - consistency regularization>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a37bbef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import easydict\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from augmentation import RandAugmentCIFAR\n",
    "from models import WideResNet, ModelEMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5fa866",
   "metadata": {},
   "source": [
    "### 1. 하이퍼파라미터세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89cab206",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "    \"seed\" : 0,\n",
    "    \"gpu\": 0,\n",
    "    \"start_step\" : 0,\n",
    "    \"total_steps\" : 2000, # 30000\n",
    "    \"eval_step\" : 20, # 100\n",
    "    \"lambda_u\" : 1,\n",
    "    \"threshold\" : 0.95,\n",
    "    \"T\" : 0.6,\n",
    "    \n",
    "    # for data\n",
    "    \"data_path\" : \"./data\",\n",
    "    \"num_data\" : 10000, # 50000\n",
    "    \"num_labeled\" : 1000,# 5000 \n",
    "    \"num_classes\" : 10, # number of classes\n",
    "    \"resize\" : 32, # resize image\n",
    "    \"batch_size\" : 64,\n",
    "    \"mu\" : 1, # coefficient of unlabeled batch size,\n",
    "    \n",
    "    # for WideResNet model\n",
    "    \"depth\" : 10,\n",
    "    \"widen_factor\" : 1,\n",
    "    \"teacher_dropout\" : 0, # dropout on last dense layer of teacher model\n",
    "    \"student_dropout\" : 0, # dropout on last dense layer of student model\n",
    "    \n",
    "    # for optimizing\n",
    "    \"lr\" : 0.01, # train learning rate of model\n",
    "    \"momentum\" : 0.9, # SGD Momentum\n",
    "    \"nesterov\" : True, # use nesterov\n",
    "    \"weight_decay\" : 0.01, # train weight decay\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a79296c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.device = torch.device('cuda', args.gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3850e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025b8ad0",
   "metadata": {},
   "source": [
    "### 2. 데이터셋 & 데이터로더 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "888cbb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "base_dataset = datasets.CIFAR10(args.data_path, train=True, download=True)\n",
    "test_dataset = datasets.CIFAR10(args.data_path, train=False, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94ad4476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_u_split(args, labels):\n",
    "    \n",
    "    label_per_class = args.num_labeled // args.num_classes\n",
    "    num_unlabel_data = ((args.num_data // args.num_classes) - label_per_class) * args.num_classes\n",
    "    # 학습 시간을 줄이기 위해서 데이터 개수를 줄이기 위해서 추가\n",
    "    \n",
    "    print(f'클래스별 labeled data 개수 : {label_per_class}')\n",
    "    print(f'Labeled data 개수 : {label_per_class * args.num_classes}')\n",
    "    print(f'Unlabeled data 개수 : {num_unlabel_data}')\n",
    "    \n",
    "    labels = np.array(labels)\n",
    "    labeled_idx = []\n",
    "    \n",
    "    unlabeled_idx = np.array(range(len(labels))) \n",
    "    for i in range(args.num_classes):\n",
    "        idx = np.where(labels == i)[0]\n",
    "        idx = np.random.choice(idx, label_per_class, False)\n",
    "        labeled_idx.extend(idx)\n",
    "    labeled_idx = np.array(labeled_idx)\n",
    "    np.random.shuffle(labeled_idx)\n",
    "    \n",
    "    unlabeled_idx = np.array([i for i in unlabeled_idx if i not in labeled_idx])\n",
    "    np.random.shuffle(unlabeled_idx)\n",
    "    unlabeled_idx = unlabeled_idx[:num_unlabel_data]\n",
    "    \n",
    "    return labeled_idx, unlabeled_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c4c6371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스별 labeled data 개수 : 100\n",
      "Labeled data 개수 : 1000\n",
      "Unlabeled data 개수 : 9000\n"
     ]
    }
   ],
   "source": [
    "labeled_idxs, unlabeled_idxs = l_u_split(args, base_dataset.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59ec4454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화에 사용될 평균, 표준편차\n",
    "cifar10_mean = (0.491400, 0.482158, 0.4465231)\n",
    "cifar10_std = (0.247032, 0.243485, 0.2615877)\n",
    "\n",
    "# Labeled 데이터셋을 위한 데이터변환 사전에 정의\n",
    "transform_labeled = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(size=args.resize,\n",
    "                              padding=int(args.resize * 0.125),\n",
    "                              fill=128,\n",
    "                              padding_mode='constant'),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=cifar10_mean, std=cifar10_std),\n",
    "    ])\n",
    "\n",
    "# Test 데이터셋을 위한 데이터변환 사전에 정의\n",
    "transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=cifar10_mean, std=cifar10_std)\n",
    "    ])\n",
    "\n",
    "# Unlabeled 데이터셋을 위한 데이터변환 사전에 정의\n",
    "# Unlabeled 데이터셋을 위한 커스터마이징된 데이터변환 클래스 만들기\n",
    "class CustomTransform(object):\n",
    "    # class 초기화\n",
    "    def __init__(self, args, mean, std):\n",
    "        n, m = 5, 10\n",
    "        \n",
    "        self.ori = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(size=args.resize,\n",
    "                                  padding=int(args.resize * 0.125),\n",
    "                                  fill=128,\n",
    "                                  padding_mode='constant')])\n",
    "        \n",
    "        self.aug = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(size=args.resize,\n",
    "                                  padding=int(args.resize * 0.125),\n",
    "                                  fill=128,\n",
    "                                  padding_mode='constant'),\n",
    "            RandAugmentCIFAR(n=n, m=m)])\n",
    "        \n",
    "        self.normalize = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=std)])\n",
    "        \n",
    "    # class가 사용될 때\n",
    "    def __call__(self, x):\n",
    "        ori = self.ori(x)\n",
    "        aug = self.aug(x)\n",
    "        return self.normalize(ori), self.normalize(aug)\n",
    "    \n",
    "transform_unlabeled = CustomTransform(args, mean=cifar10_mean, std=cifar10_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bd8ba00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCIFAR10SSL(datasets.CIFAR10):\n",
    "    def __init__(self, root, indexs, train=True,\n",
    "                 transform=None, target_transform=None, download=False):\n",
    "        super().__init__(root, train=train,\n",
    "                         transform=transform,\n",
    "                         target_transform=target_transform,\n",
    "                         download=download)\n",
    "        if indexs is not None:\n",
    "            self.data = self.data[indexs]\n",
    "            self.targets = np.array(self.targets)[indexs]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed828c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_dataset = CustomCIFAR10SSL(args.data_path, labeled_idxs, train=True, transform=transform_labeled)\n",
    "unlabeled_dataset = CustomCIFAR10SSL(args.data_path, unlabeled_idxs, train=True, \n",
    "                                     transform=transform_unlabeled)\n",
    "test_dataset = datasets.CIFAR10(args.data_path, train=False, transform=transform_test, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efbf8926",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_loader = DataLoader(labeled_dataset, sampler=RandomSampler(labeled_dataset),\n",
    "                            batch_size=args.batch_size, drop_last=True)\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, sampler=RandomSampler(unlabeled_dataset),\n",
    "                              batch_size=args.batch_size * args.mu, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767b68f5",
   "metadata": {},
   "source": [
    "### 3. Labeled & unlabeled 데이터셋을 사용한 semi-supervised learning\n",
    "### 3-1 Consistency regularization 예시1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb2ae980",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WideResNet(num_classes=args.num_classes,\n",
    "                   depth=args.depth,\n",
    "                   widen_factor=args.widen_factor,\n",
    "                   dropout=0,\n",
    "                   dense_dropout=args.teacher_dropout)\n",
    "model.to(args.device)\n",
    "optimizer = optim.SGD(model.parameters(),\n",
    "                      lr=args.lr,\n",
    "                      momentum=args.momentum,\n",
    "                      nesterov=args.nesterov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d917910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_consistency_regularization(args, model, optimizer):\n",
    "    since = time.time()\n",
    "    for step in range(args.start_step, args.total_steps):\n",
    "        if step % args.eval_step == 0:\n",
    "            if step != 0:\n",
    "                print('{} Step - loss: {:.4f} cross entropy : {:.4f} consistency reg : {:.4f}'.format(step,\n",
    "                                                                                                      np.mean(losses), \n",
    "                                                                                                      np.mean(ce_losses), \n",
    "                                                                                                      np.mean(cr_losses)))\n",
    "        \n",
    "            losses = []\n",
    "            ce_losses = []\n",
    "            cr_losses = []\n",
    "            \n",
    "        model.train()\n",
    "\n",
    "        try:\n",
    "            images_l, targets = labeled_iter.next()\n",
    "        except:\n",
    "            labeled_iter = iter(labeled_loader)\n",
    "            images_l, targets = labeled_iter.next()\n",
    "\n",
    "        try:\n",
    "            (images_uw, images_us), _ = unlabeled_iter.next()\n",
    "        except:\n",
    "            unlabeled_iter = iter(unlabeled_loader)\n",
    "            (images_uw, images_us), _ = unlabeled_iter.next()\n",
    "\n",
    "        images_l = images_l.to(args.device)\n",
    "        images_uw = images_uw.to(args.device)\n",
    "        images_us = images_us.to(args.device)\n",
    "        targets = targets.to(args.device, dtype=torch.long)\n",
    "\n",
    "        # parameter gradients를 0으로 설정\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward model\n",
    "        batch_size = images_l.shape[0]\n",
    "        images = torch.cat((images_l, images_uw, images_us))\n",
    "        logits = model(images)\n",
    "        logits_l = logits[:batch_size]\n",
    "        logits_uw, logits_us = logits[batch_size:].chunk(2)\n",
    "        del logits\n",
    "\n",
    "        loss_l = F.cross_entropy(logits_l, targets, reduction='mean')\n",
    "\n",
    "        # make pseudo label\n",
    "        soft_pseudo_label = torch.softmax(logits_uw.detach()/args.T, dim=-1)\n",
    "        max_probs, hard_pseudo_label = torch.max(soft_pseudo_label, dim=-1)\n",
    "        mask = max_probs.ge(args.threshold).float()\n",
    "        \n",
    "        loss_u = (-(soft_pseudo_label * torch.log_softmax(logits_us, dim=-1)).sum(dim=-1) * mask).mean()\n",
    "        #loss_u = (((soft_pseudo_label - torch.log_softmax(logits_us, dim=-1))**2).sum(dim=-1) * mask).mean()\n",
    "        loss = loss_l + args.lambda_u * loss_u\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        ce_losses.append(loss_l.item())\n",
    "        cr_losses.append(loss_u.item())\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f417d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 Step - loss: 2.2874 cross entropy : 2.2874 consistency reg : 0.0000\n",
      "40 Step - loss: 2.0385 cross entropy : 2.0366 consistency reg : 0.0019\n",
      "60 Step - loss: 1.9537 cross entropy : 1.9537 consistency reg : 0.0000\n",
      "80 Step - loss: 1.8988 cross entropy : 1.8954 consistency reg : 0.0033\n",
      "100 Step - loss: 1.8394 cross entropy : 1.8384 consistency reg : 0.0010\n",
      "120 Step - loss: 1.8435 cross entropy : 1.8408 consistency reg : 0.0028\n",
      "140 Step - loss: 1.7899 cross entropy : 1.7887 consistency reg : 0.0013\n",
      "160 Step - loss: 1.7733 cross entropy : 1.7694 consistency reg : 0.0039\n",
      "180 Step - loss: 1.7618 cross entropy : 1.7586 consistency reg : 0.0032\n",
      "200 Step - loss: 1.7248 cross entropy : 1.7200 consistency reg : 0.0048\n",
      "220 Step - loss: 1.6829 cross entropy : 1.6807 consistency reg : 0.0022\n",
      "240 Step - loss: 1.6999 cross entropy : 1.6862 consistency reg : 0.0136\n",
      "260 Step - loss: 1.6755 cross entropy : 1.6683 consistency reg : 0.0071\n",
      "280 Step - loss: 1.6564 cross entropy : 1.6541 consistency reg : 0.0023\n",
      "300 Step - loss: 1.6311 cross entropy : 1.6274 consistency reg : 0.0038\n",
      "320 Step - loss: 1.6134 cross entropy : 1.5981 consistency reg : 0.0153\n",
      "340 Step - loss: 1.6404 cross entropy : 1.6258 consistency reg : 0.0146\n",
      "360 Step - loss: 1.5828 cross entropy : 1.5718 consistency reg : 0.0110\n",
      "380 Step - loss: 1.5799 cross entropy : 1.5644 consistency reg : 0.0155\n",
      "400 Step - loss: 1.5198 cross entropy : 1.5128 consistency reg : 0.0070\n",
      "420 Step - loss: 1.5575 cross entropy : 1.5336 consistency reg : 0.0239\n",
      "440 Step - loss: 1.5441 cross entropy : 1.5186 consistency reg : 0.0256\n",
      "460 Step - loss: 1.5097 cross entropy : 1.4851 consistency reg : 0.0246\n",
      "480 Step - loss: 1.5315 cross entropy : 1.5048 consistency reg : 0.0267\n",
      "500 Step - loss: 1.4630 cross entropy : 1.4409 consistency reg : 0.0221\n",
      "520 Step - loss: 1.4979 cross entropy : 1.4702 consistency reg : 0.0277\n",
      "540 Step - loss: 1.4630 cross entropy : 1.4298 consistency reg : 0.0332\n",
      "560 Step - loss: 1.4510 cross entropy : 1.4205 consistency reg : 0.0305\n",
      "580 Step - loss: 1.4675 cross entropy : 1.4211 consistency reg : 0.0464\n",
      "600 Step - loss: 1.4273 cross entropy : 1.3631 consistency reg : 0.0642\n",
      "620 Step - loss: 1.4262 cross entropy : 1.3809 consistency reg : 0.0454\n",
      "640 Step - loss: 1.4095 cross entropy : 1.3794 consistency reg : 0.0301\n",
      "660 Step - loss: 1.4230 cross entropy : 1.3569 consistency reg : 0.0661\n",
      "680 Step - loss: 1.3874 cross entropy : 1.3337 consistency reg : 0.0537\n",
      "700 Step - loss: 1.4052 cross entropy : 1.3652 consistency reg : 0.0400\n",
      "720 Step - loss: 1.3713 cross entropy : 1.2996 consistency reg : 0.0717\n",
      "740 Step - loss: 1.3819 cross entropy : 1.3104 consistency reg : 0.0716\n",
      "760 Step - loss: 1.4028 cross entropy : 1.3327 consistency reg : 0.0701\n",
      "780 Step - loss: 1.3114 cross entropy : 1.2374 consistency reg : 0.0740\n",
      "800 Step - loss: 1.3627 cross entropy : 1.2720 consistency reg : 0.0907\n",
      "820 Step - loss: 1.3529 cross entropy : 1.2417 consistency reg : 0.1112\n",
      "840 Step - loss: 1.3522 cross entropy : 1.2466 consistency reg : 0.1056\n",
      "860 Step - loss: 1.3526 cross entropy : 1.2566 consistency reg : 0.0959\n",
      "880 Step - loss: 1.3204 cross entropy : 1.2134 consistency reg : 0.1069\n",
      "900 Step - loss: 1.3650 cross entropy : 1.2505 consistency reg : 0.1145\n",
      "920 Step - loss: 1.3155 cross entropy : 1.1993 consistency reg : 0.1162\n",
      "940 Step - loss: 1.2633 cross entropy : 1.1734 consistency reg : 0.0899\n",
      "960 Step - loss: 1.3319 cross entropy : 1.2032 consistency reg : 0.1287\n",
      "980 Step - loss: 1.2446 cross entropy : 1.1484 consistency reg : 0.0962\n",
      "1000 Step - loss: 1.2586 cross entropy : 1.1670 consistency reg : 0.0916\n",
      "1020 Step - loss: 1.2870 cross entropy : 1.1445 consistency reg : 0.1425\n",
      "1040 Step - loss: 1.2619 cross entropy : 1.1237 consistency reg : 0.1381\n",
      "1060 Step - loss: 1.2934 cross entropy : 1.1325 consistency reg : 0.1609\n",
      "1080 Step - loss: 1.2649 cross entropy : 1.1220 consistency reg : 0.1428\n",
      "1100 Step - loss: 1.2684 cross entropy : 1.1247 consistency reg : 0.1437\n",
      "1120 Step - loss: 1.2759 cross entropy : 1.1250 consistency reg : 0.1509\n",
      "1140 Step - loss: 1.2188 cross entropy : 1.0729 consistency reg : 0.1459\n",
      "1160 Step - loss: 1.2868 cross entropy : 1.1133 consistency reg : 0.1735\n",
      "1180 Step - loss: 1.2217 cross entropy : 1.0550 consistency reg : 0.1668\n",
      "1200 Step - loss: 1.2239 cross entropy : 1.0692 consistency reg : 0.1546\n",
      "1220 Step - loss: 1.2176 cross entropy : 1.0489 consistency reg : 0.1687\n",
      "1240 Step - loss: 1.2331 cross entropy : 1.0644 consistency reg : 0.1687\n",
      "1260 Step - loss: 1.2173 cross entropy : 1.0486 consistency reg : 0.1687\n",
      "1280 Step - loss: 1.1949 cross entropy : 1.0389 consistency reg : 0.1560\n",
      "1300 Step - loss: 1.2162 cross entropy : 1.0088 consistency reg : 0.2074\n",
      "1320 Step - loss: 1.2396 cross entropy : 1.0594 consistency reg : 0.1802\n",
      "1340 Step - loss: 1.1877 cross entropy : 0.9929 consistency reg : 0.1948\n",
      "1360 Step - loss: 1.2273 cross entropy : 1.0386 consistency reg : 0.1888\n",
      "1380 Step - loss: 1.1978 cross entropy : 1.0248 consistency reg : 0.1730\n",
      "1400 Step - loss: 1.1675 cross entropy : 0.9734 consistency reg : 0.1941\n",
      "1420 Step - loss: 1.2189 cross entropy : 1.0117 consistency reg : 0.2072\n",
      "1440 Step - loss: 1.1890 cross entropy : 0.9586 consistency reg : 0.2304\n",
      "1460 Step - loss: 1.2545 cross entropy : 1.0148 consistency reg : 0.2397\n",
      "1480 Step - loss: 1.1057 cross entropy : 0.9334 consistency reg : 0.1723\n",
      "1500 Step - loss: 1.1730 cross entropy : 0.9712 consistency reg : 0.2017\n",
      "1520 Step - loss: 1.1911 cross entropy : 0.9362 consistency reg : 0.2548\n",
      "1540 Step - loss: 1.1330 cross entropy : 0.9230 consistency reg : 0.2100\n",
      "1560 Step - loss: 1.2162 cross entropy : 0.9654 consistency reg : 0.2508\n",
      "1580 Step - loss: 1.1587 cross entropy : 0.9103 consistency reg : 0.2484\n",
      "1600 Step - loss: 1.1938 cross entropy : 0.9389 consistency reg : 0.2549\n",
      "1620 Step - loss: 1.0972 cross entropy : 0.9088 consistency reg : 0.1884\n",
      "1640 Step - loss: 1.1389 cross entropy : 0.8781 consistency reg : 0.2608\n",
      "1660 Step - loss: 1.1340 cross entropy : 0.8820 consistency reg : 0.2521\n",
      "1680 Step - loss: 1.1086 cross entropy : 0.9066 consistency reg : 0.2020\n",
      "1700 Step - loss: 1.1382 cross entropy : 0.8584 consistency reg : 0.2798\n",
      "1720 Step - loss: 1.0840 cross entropy : 0.8426 consistency reg : 0.2413\n",
      "1740 Step - loss: 1.0871 cross entropy : 0.8665 consistency reg : 0.2206\n",
      "1760 Step - loss: 1.1167 cross entropy : 0.8260 consistency reg : 0.2907\n",
      "1780 Step - loss: 1.1539 cross entropy : 0.8483 consistency reg : 0.3056\n",
      "1800 Step - loss: 1.1659 cross entropy : 0.8775 consistency reg : 0.2884\n",
      "1820 Step - loss: 1.1466 cross entropy : 0.8453 consistency reg : 0.3014\n",
      "1840 Step - loss: 1.0206 cross entropy : 0.7851 consistency reg : 0.2354\n",
      "1860 Step - loss: 1.1464 cross entropy : 0.8303 consistency reg : 0.3161\n",
      "1880 Step - loss: 1.0853 cross entropy : 0.8181 consistency reg : 0.2672\n",
      "1900 Step - loss: 1.0950 cross entropy : 0.7924 consistency reg : 0.3026\n",
      "1920 Step - loss: 1.1021 cross entropy : 0.8238 consistency reg : 0.2782\n",
      "1940 Step - loss: 1.0377 cross entropy : 0.7683 consistency reg : 0.2694\n",
      "1960 Step - loss: 1.1450 cross entropy : 0.8391 consistency reg : 0.3059\n",
      "1980 Step - loss: 1.0762 cross entropy : 0.7673 consistency reg : 0.3089\n",
      "Training complete in 2m 53s\n"
     ]
    }
   ],
   "source": [
    "train_consistency_regularization(args, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9a82a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        corrects = 0\n",
    "        total = 0\n",
    "        for inputs, targets in loader:\n",
    "            inputs = inputs.to(args.device)\n",
    "            targets = targets.to(args.device, dtype=torch.long)\n",
    "\n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # output 중 최대값의 위치에 해당하는 class로 예측 수행\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            # batch별 정답 개수를 축적함\n",
    "            corrects += torch.sum(preds == targets.data)\n",
    "            total += targets.size(0)\n",
    "\n",
    "    test_acc = corrects.double() / total\n",
    "    print('Testing Acc: {:.4f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f54c8aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Acc: 0.4319\n"
     ]
    }
   ],
   "source": [
    "test(args, model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
