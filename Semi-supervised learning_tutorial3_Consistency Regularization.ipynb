{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d2f04a7",
   "metadata": {},
   "source": [
    "# <Semi-supervised learning tutorial 3 - consistency regularization>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85773cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'LG_semi_supervised_learning_day17' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/KU-DIC/LG_semi_supervised_learning_day17.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a37bbef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import easydict\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from LG_semi_supervised_learning_day17.augmentation import RandAugmentCIFAR\n",
    "from LG_semi_supervised_learning_day17.models import WideResNet, ModelEMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab0a8c3",
   "metadata": {},
   "source": [
    "### 하이퍼파라미터세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89cab206",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "    \"seed\" : 0,\n",
    "    \"gpu\": 0,\n",
    "    \"start_step\" : 0,\n",
    "    \"total_steps\" : 2000, # 30000\n",
    "    \"eval_step\" : 20, # 100\n",
    "    \"lambda_u\" : 10,\n",
    "    \n",
    "    # for data\n",
    "    \"data_path\" : \"/content/LG_semi_supervised_learning_day17/data\",\n",
    "    \"num_data\" : 10000, # 50000\n",
    "    \"num_labeled\" : 1000,# 5000 \n",
    "    \"num_classes\" : 10, # number of classes\n",
    "    \"resize\" : 32, # resize image\n",
    "    \"batch_size\" : 64,\n",
    "    \"mu\" : 1, # coefficient of unlabeled batch size,\n",
    "    \n",
    "    # for WideResNet model\n",
    "    \"depth\" : 10,\n",
    "    \"widen_factor\" : 1,\n",
    "    \"teacher_dropout\" : 0, # dropout on last dense layer of teacher model\n",
    "    \"student_dropout\" : 0, # dropout on last dense layer of student model\n",
    "    \n",
    "    # for optimizing\n",
    "    \"lr\" : 0.01, # train learning rate of model\n",
    "    \"momentum\" : 0.9, # SGD Momentum\n",
    "    \"nesterov\" : True, # use nesterov\n",
    "    \"weight_decay\" : 0.01, # train weight decay\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a79296c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.device = torch.device('cuda', args.gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3850e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907bd8a5",
   "metadata": {},
   "source": [
    "### 데이터셋 & 데이터로더 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "888cbb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "base_dataset = datasets.CIFAR10(args.data_path, train=True, download=True)\n",
    "test_dataset = datasets.CIFAR10(args.data_path, train=False, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94ad4476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_u_split(args, labels):\n",
    "    \n",
    "    label_per_class = args.num_labeled // args.num_classes\n",
    "    num_unlabel_data = ((args.num_data // args.num_classes) - label_per_class) * args.num_classes\n",
    "    # 학습 시간을 줄이기 위해서 데이터 개수를 줄이기 위해서 추가\n",
    "    \n",
    "    print(f'클래스별 labeled data 개수 : {label_per_class}')\n",
    "    print(f'Labeled data 개수 : {label_per_class * args.num_classes}')\n",
    "    print(f'Unlabeled data 개수 : {num_unlabel_data}')\n",
    "    \n",
    "    labels = np.array(labels)\n",
    "    labeled_idx = []\n",
    "    \n",
    "    unlabeled_idx = np.array(range(len(labels))) \n",
    "    for i in range(args.num_classes):\n",
    "        idx = np.where(labels == i)[0]\n",
    "        idx = np.random.choice(idx, label_per_class, False)\n",
    "        labeled_idx.extend(idx)\n",
    "    labeled_idx = np.array(labeled_idx)\n",
    "    np.random.shuffle(labeled_idx)\n",
    "    \n",
    "    unlabeled_idx = np.array([i for i in unlabeled_idx if i not in labeled_idx])\n",
    "    np.random.shuffle(unlabeled_idx)\n",
    "    unlabeled_idx = unlabeled_idx[:num_unlabel_data]\n",
    "    \n",
    "    return labeled_idx, unlabeled_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c4c6371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스별 labeled data 개수 : 100\n",
      "Labeled data 개수 : 1000\n",
      "Unlabeled data 개수 : 9000\n"
     ]
    }
   ],
   "source": [
    "labeled_idxs, unlabeled_idxs = l_u_split(args, base_dataset.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59ec4454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화에 사용될 평균, 표준편차\n",
    "cifar10_mean = (0.491400, 0.482158, 0.4465231)\n",
    "cifar10_std = (0.247032, 0.243485, 0.2615877)\n",
    "\n",
    "# # 데이터셋을 위한 커스터마이징된 데이터변환 클래스 만들기\n",
    "class CustomTransform(object):\n",
    "    # class 초기화\n",
    "    def __init__(self, args, n, m, mean, std):\n",
    "        self.n, self.m = n, m\n",
    "        \n",
    "        self.ori = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(size=args.resize,\n",
    "                                  padding=int(args.resize * 0.125),\n",
    "                                  fill=128,\n",
    "                                  padding_mode='constant')])\n",
    "        \n",
    "        self.aug = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(size=args.resize,\n",
    "                                  padding=int(args.resize * 0.125),\n",
    "                                  fill=128,\n",
    "                                  padding_mode='constant'),\n",
    "            RandAugmentCIFAR(n=n, m=m)])\n",
    "        \n",
    "        self.normalize = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=std)])\n",
    "        \n",
    "    # class가 사용될 때\n",
    "    def __call__(self, x):\n",
    "        ori = self.ori(x)\n",
    "        aug = self.aug(x)\n",
    "        return self.normalize(ori), self.normalize(aug)\n",
    "    \n",
    "# Labeled & Unlabeled 데이터셋을 위한 데이터변환 사전에 정의\n",
    "transform_labeled = CustomTransform(args, n=5, m=20, mean=cifar10_mean, std=cifar10_std)\n",
    "transform_unlabeled = CustomTransform(args, n=5, m=20, mean=cifar10_mean, std=cifar10_std)\n",
    "\n",
    "# Test 데이터셋을 위한 데이터변환 사전에 정의\n",
    "transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=cifar10_mean, std=cifar10_std)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bd8ba00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCIFAR10SSL(datasets.CIFAR10):\n",
    "    def __init__(self, root, indexs, train=True,\n",
    "                 transform=None, target_transform=None, download=False):\n",
    "        super().__init__(root, train=train,\n",
    "                         transform=transform,\n",
    "                         target_transform=target_transform,\n",
    "                         download=download)\n",
    "        if indexs is not None:\n",
    "            self.data = self.data[indexs]\n",
    "            self.targets = np.array(self.targets)[indexs]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed828c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_dataset = CustomCIFAR10SSL(args.data_path, \n",
    "                                   labeled_idxs, \n",
    "                                   train=True, \n",
    "                                   transform=transform_labeled)\n",
    "unlabeled_dataset = CustomCIFAR10SSL(args.data_path, \n",
    "                                     unlabeled_idxs, \n",
    "                                     train=True, \n",
    "                                     transform=transform_unlabeled)\n",
    "test_dataset = datasets.CIFAR10(args.data_path, train=False, transform=transform_test, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efbf8926",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_loader = DataLoader(labeled_dataset, sampler=RandomSampler(labeled_dataset),\n",
    "                            batch_size=args.batch_size, drop_last=True)\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, sampler=RandomSampler(unlabeled_dataset),\n",
    "                              batch_size=args.batch_size * args.mu, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767b68f5",
   "metadata": {},
   "source": [
    "### Labeled & unlabeled 데이터셋을 사용한 semi-supervised learning\n",
    "#### Consistency regularization 예시1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb2ae980",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WideResNet(num_classes=args.num_classes,\n",
    "                   depth=args.depth,\n",
    "                   widen_factor=args.widen_factor,\n",
    "                   dropout=0,\n",
    "                   dense_dropout=args.teacher_dropout)\n",
    "model.to(args.device)\n",
    "optimizer = optim.SGD(model.parameters(),\n",
    "                      lr=args.lr,\n",
    "                      momentum=args.momentum,\n",
    "                      nesterov=args.nesterov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d917910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_consistency_regularization_ex1(args, model, optimizer):\n",
    "    since = time.time()\n",
    "    for step in range(args.start_step, args.total_steps):\n",
    "        if step % args.eval_step == 0:\n",
    "            if step != 0:\n",
    "                print('{} Step - loss: {:.4f} '.format(step,np.mean(losses)))\n",
    "        \n",
    "            losses = []\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        try:\n",
    "            (images_lw, images_ls), targets = labeled_iter.next()\n",
    "        except:\n",
    "            labeled_iter = iter(labeled_loader)\n",
    "            (images_lw, images_ls), targets = labeled_iter.next()\n",
    "\n",
    "        try:\n",
    "            (images_uw, images_us), _ = unlabeled_iter.next()\n",
    "        except:\n",
    "            unlabeled_iter = iter(unlabeled_loader)\n",
    "            (images_uw, images_us), _ = unlabeled_iter.next()\n",
    "\n",
    "        images_lw = images_lw.to(args.device)\n",
    "        images_ls = images_ls.to(args.device)\n",
    "        images_uw = images_uw.to(args.device)\n",
    "        images_us = images_us.to(args.device)\n",
    "        targets = targets.to(args.device, dtype=torch.long)\n",
    "\n",
    "        # parameter gradients를 0으로 설정\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward model\n",
    "        batch_size = images_ls.shape[0]\n",
    "        images = torch.cat((images_ls, images_lw, images_uw, images_us))\n",
    "        logits = model(images)\n",
    "        logits_ls = logits[:batch_size]\n",
    "        logits_lw = logits[batch_size:2*batch_size]\n",
    "        logits_uw, logits_us = logits[2*batch_size:].chunk(2)\n",
    "        del logits\n",
    "        \n",
    "        # loss for for labeled data\n",
    "        ls_pred_prob = torch.softmax(logits_ls, dim=-1)\n",
    "        lw_pred_prob = torch.softmax(logits_lw, dim=-1)\n",
    "        \n",
    "        loss_l_u = F.mse_loss(ls_pred_prob,lw_pred_prob, reduction='mean')\n",
    "        loss_l_s = F.cross_entropy(logits_lw, targets, reduction='mean')\n",
    "\n",
    "        # loss for for unlabeled data\n",
    "        us_pred_prob = torch.softmax(logits_us, dim=-1)\n",
    "        uw_pred_prob = torch.softmax(logits_uw, dim=-1)\n",
    "        loss_u_u = F.mse_loss(us_pred_prob,uw_pred_prob, reduction='mean')\n",
    "        \n",
    "        loss_u = (loss_l_u + loss_u_u)/2\n",
    "        \n",
    "        # total loss\n",
    "        loss = loss_l_s + args.lambda_u * loss_u\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f417d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 Step - loss: 2.2999 \n",
      "40 Step - loss: 2.0902 \n",
      "60 Step - loss: 2.0281 \n",
      "80 Step - loss: 1.9707 \n",
      "100 Step - loss: 1.9494 \n",
      "120 Step - loss: 1.9058 \n",
      "140 Step - loss: 1.8852 \n",
      "160 Step - loss: 1.8748 \n",
      "180 Step - loss: 1.8699 \n"
     ]
    }
   ],
   "source": [
    "train_consistency_regularization_ex1(args, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a82a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        corrects = 0\n",
    "        total = 0\n",
    "        for inputs, targets in loader:\n",
    "            inputs = inputs.to(args.device)\n",
    "            targets = targets.to(args.device, dtype=torch.long)\n",
    "\n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # output 중 최대값의 위치에 해당하는 class로 예측 수행\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            # batch별 정답 개수를 축적함\n",
    "            corrects += torch.sum(preds == targets.data)\n",
    "            total += targets.size(0)\n",
    "\n",
    "    test_acc = corrects.double() / total\n",
    "    print('Testing Acc: {:.4f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54c8aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(args, model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e822d519",
   "metadata": {},
   "source": [
    "#### Consistency regularization 예시2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1e63d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_labeled = CustomTransform(args, n=2, m=10, mean=cifar10_mean, std=cifar10_std)\n",
    "transform_unlabeled = CustomTransform(args, n=2, m=10, mean=cifar10_mean, std=cifar10_std)\n",
    "labeled_dataset = CustomCIFAR10SSL(args.data_path, \n",
    "                                   labeled_idxs, \n",
    "                                   train=True, \n",
    "                                   transform=transform_labeled)\n",
    "unlabeled_dataset = CustomCIFAR10SSL(args.data_path, \n",
    "                                     unlabeled_idxs, \n",
    "                                     train=True, \n",
    "                                     transform=transform_unlabeled)\n",
    "labeled_loader = DataLoader(labeled_dataset, sampler=RandomSampler(labeled_dataset),\n",
    "                            batch_size=args.batch_size, drop_last=True)\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, sampler=RandomSampler(unlabeled_dataset),\n",
    "                              batch_size=args.batch_size * args.mu, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfd04fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args.tau = 0.4\n",
    "args.beta = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca96ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WideResNet(num_classes=args.num_classes,\n",
    "                   depth=args.depth,\n",
    "                   widen_factor=args.widen_factor,\n",
    "                   dropout=0,\n",
    "                   dense_dropout=args.teacher_dropout)\n",
    "model.to(args.device)\n",
    "optimizer = optim.SGD(model.parameters(),\n",
    "                      lr=args.lr,\n",
    "                      momentum=args.momentum,\n",
    "                      nesterov=args.nesterov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc7907e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_consistency_regularization_ex2(args, model, optimizer):\n",
    "    since = time.time()\n",
    "    for step in range(args.start_step, args.total_steps):\n",
    "        if step % args.eval_step == 0:\n",
    "            if step != 0:\n",
    "                print('{} Step - loss: {:.4f} '.format(step,np.mean(losses)))\n",
    "        \n",
    "            losses = []\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        try:\n",
    "            (images_l, _ ), targets = labeled_iter.next()\n",
    "        except:\n",
    "            labeled_iter = iter(labeled_loader)\n",
    "            (images_l, _ ), targets = labeled_iter.next()\n",
    "\n",
    "        try:\n",
    "            (images_uw, images_us), _ = unlabeled_iter.next()\n",
    "        except:\n",
    "            unlabeled_iter = iter(unlabeled_loader)\n",
    "            (images_uw, images_us), _ = unlabeled_iter.next()\n",
    "\n",
    "        images_l = images_l.to(args.device)\n",
    "        images_uw = images_uw.to(args.device)\n",
    "        images_us = images_us.to(args.device)\n",
    "        targets = targets.to(args.device, dtype=torch.long)\n",
    "\n",
    "        # parameter gradients를 0으로 설정\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward model\n",
    "        batch_size = images_l.shape[0]\n",
    "        images = torch.cat((images_l, images_uw, images_us))\n",
    "        logits = model(images)\n",
    "        logits_l = logits[:batch_size]\n",
    "        logits_uw, logits_us = logits[batch_size:].chunk(2)\n",
    "        del logits\n",
    "        \n",
    "        # loss for for labeled data\n",
    "        loss_l = F.cross_entropy(logits_l, targets, reduction='mean')\n",
    "\n",
    "        # loss for for unlabeled data\n",
    "        targets_u = torch.softmax(logits_uw.detach()/args.tau, dim=-1)\n",
    "        max_probs, _  = torch.max(targets_u, dim=-1)\n",
    "        mask = max_probs.ge(args.beta).float()\n",
    "        \n",
    "        loss_u = (-(targets_u * torch.log_softmax(logits_us, dim=-1)).sum(dim=-1) * mask).mean()\n",
    "        \n",
    "        # total loss\n",
    "        loss = loss_l + args.lambda_u * loss_u\n",
    "\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1098aeeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_consistency_regularization_ex2(args, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b187cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test(args, model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
