{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67587e99",
   "metadata": {},
   "source": [
    "# <Semi-supervised learning tutorial 2 - pseudo labeling>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0c4bde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! git clone https://github.com/KU-DIC/LG_semi-supervised_learning_day17.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "039e69bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "import easydict# dictionary의 속성을 dot(.)을 사용하여 표기가능\n",
    "from tqdm.auto import tqdm #ipython파일에서 출력을 깔끔하게하기위해 tqdm.tqdm 대신 tqdm.auto.tqdm 또는 tqdm.notebook.tqdm 사용\n",
    "from PIL import Image # PIL(Python Imaging Library)\n",
    "\n",
    "from augmentation import RandAugmentCIFAR # 데이터 증강에 필요한 함수 작성해 모아놓은 augmentation.py 파일\n",
    "from models import WideResNet # 모델 관련 함수 작성해 모아놓은 models.py 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1c32d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "    \"seed\" : 0,\n",
    "    \"gpu\": 0,\n",
    "    \"start_step\" : 0,\n",
    "    \"total_steps\" : 2000, # 300000\n",
    "    \"eval_step\" : 20, # 100\n",
    "    \"lambda_u\" : 1,\n",
    "    \n",
    "    # for supervised learning\n",
    "    \"total_epoch\" : 100,\n",
    "    \n",
    "    # for data\n",
    "    \"data_path\" : \"./data\",\n",
    "    \"num_data\" : 10000, # 50000\n",
    "    \"num_labeled\" : 1000,# 5000 \n",
    "    \"num_classes\" : 10, # number of classes\n",
    "    \"resize\" : 32, # resize image\n",
    "    \"batch_size\" : 64,\n",
    "    \"mu\" : 1, # coefficient of unlabeled batch size,\n",
    "    \n",
    "    # for WideResNet model\n",
    "    \"depth\" : 10, # 기본 28, assert((depth - 4) % 6 == 0) 학습 시간을 줄이기 위해서 모델 크기 줄임\n",
    "    \"widen_factor\" : 1, # 기본 2, , 학습 시간을 줄이기 위해서 모델 크기 줄임\n",
    "    \"teacher_dropout\" : 0, # dropout on last dense layer of teacher model\n",
    "    \"student_dropout\" : 0, # dropout on last dense layer of student model\n",
    "    \n",
    "    # for optimizing\n",
    "    \"teacher_lr\" : 0.01, # train learning rate of teacher model\n",
    "    \"student_lr\" : 0.01, # train learning rate of student model\n",
    "    \"momentum\" : 0.9, # SGD Momentum\n",
    "    \"nesterov\" : True, # use nesterov\n",
    "    \"weight_decay\" : 0.01, # train weight decay\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89a3c25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.device = torch.device('cuda', args.gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9184cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f687fd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "base_dataset = datasets.CIFAR10(args.data_path, train=True, download=True)\n",
    "test_dataset = datasets.CIFAR10(args.data_path, train=False, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5df47c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_u_split(args, labels):\n",
    "    \n",
    "    label_per_class = args.num_labeled // args.num_classes\n",
    "    num_unlabel_data = ((args.num_data // args.num_classes) - label_per_class) * args.num_classes\n",
    "    # 학습 시간을 줄이기 위해서 데이터 개수를 줄이기 위해서 추가\n",
    "    \n",
    "    print(f'클래스별 labeled data 개수 : {label_per_class}')\n",
    "    print(f'Labeled data 개수 : {label_per_class * args.num_classes}')\n",
    "    print(f'Unlabeled data 개수 : {num_unlabel_data}')\n",
    "    \n",
    "    labels = np.array(labels)\n",
    "    labeled_idx = []\n",
    "    \n",
    "    unlabeled_idx = np.array(range(len(labels))) \n",
    "    for i in range(args.num_classes):\n",
    "        idx = np.where(labels == i)[0]\n",
    "        idx = np.random.choice(idx, label_per_class, False)\n",
    "        labeled_idx.extend(idx)\n",
    "    labeled_idx = np.array(labeled_idx)\n",
    "    np.random.shuffle(labeled_idx)\n",
    "    \n",
    "    unlabeled_idx = np.array([i for i in unlabeled_idx if i not in labeled_idx])\n",
    "    np.random.shuffle(unlabeled_idx)\n",
    "    unlabeled_idx = unlabeled_idx[:num_unlabel_data]\n",
    "    \n",
    "    return labeled_idx, unlabeled_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8cecad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "클래스별 labeled data 개수 : 100\n",
      "Labeled data 개수 : 1000\n",
      "Unlabeled data 개수 : 9000\n"
     ]
    }
   ],
   "source": [
    "labeled_idxs, unlabeled_idxs = l_u_split(args, base_dataset.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc55a11e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labeled_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e80bcd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unlabeled_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5cc4d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화에 사용될 평균, 표준편차\n",
    "cifar10_mean = (0.491400, 0.482158, 0.4465231)\n",
    "cifar10_std = (0.247032, 0.243485, 0.2615877)\n",
    "\n",
    "# Labeled 데이터셋을 위한 데이터변환 사전에 정의\n",
    "transform_labeled = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(size=args.resize,\n",
    "                              padding=int(args.resize * 0.125),\n",
    "                              fill=128,\n",
    "                              padding_mode='constant'),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=cifar10_mean, std=cifar10_std),\n",
    "    ])\n",
    "\n",
    "# Test 데이터셋을 위한 데이터변환 사전에 정의\n",
    "transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=cifar10_mean, std=cifar10_std)\n",
    "    ])\n",
    "\n",
    "# Unlabeled 데이터셋을 위한 데이터변환 사전에 정의\n",
    "# Unlabeled 데이터셋을 위한 커스터마이징된 데이터변환 클래스 만들기\n",
    "class CustomTransform(object):\n",
    "    # class 초기화\n",
    "    def __init__(self, args, n, m, mean, std):\n",
    "        self.n, self.m = n, m\n",
    "        \n",
    "        self.ori = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(size=args.resize,\n",
    "                                  padding=int(args.resize * 0.125),\n",
    "                                  fill=128,\n",
    "                                  padding_mode='constant')])\n",
    "        \n",
    "        self.aug = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(size=args.resize,\n",
    "                                  padding=int(args.resize * 0.125),\n",
    "                                  fill=128,\n",
    "                                  padding_mode='constant'),\n",
    "            RandAugmentCIFAR(n=n, m=m)])\n",
    "        \n",
    "        self.normalize = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=mean, std=std)])\n",
    "        \n",
    "    # class가 사용될 때\n",
    "    def __call__(self, x):\n",
    "        ori = self.ori(x)\n",
    "        aug = self.aug(x)\n",
    "        return self.normalize(ori), self.normalize(aug)\n",
    "    \n",
    "transform_unlabeled = CustomTransform(args, n=5, m=10, mean=cifar10_mean, std=cifar10_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd179c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCIFAR10SSL(datasets.CIFAR10):\n",
    "    def __init__(self, root, indexs, train=True,\n",
    "                 transform=None, target_transform=None, download=False):\n",
    "        super().__init__(root, train=train,\n",
    "                         transform=transform,\n",
    "                         target_transform=target_transform,\n",
    "                         download=download)\n",
    "        if indexs is not None:\n",
    "            self.data = self.data[indexs]\n",
    "            self.targets = np.array(self.targets)[indexs]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cd15144",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_dataset = CustomCIFAR10SSL(args.data_path, labeled_idxs, train=True, transform=transform_labeled)\n",
    "unlabeled_dataset = CustomCIFAR10SSL(args.data_path, unlabeled_idxs, train=True, \n",
    "                                     transform=transform_unlabeled)\n",
    "test_dataset = datasets.CIFAR10(args.data_path, train=False, transform=transform_test, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75c4303c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_loader = DataLoader(labeled_dataset, sampler=RandomSampler(labeled_dataset),\n",
    "                            batch_size=args.batch_size, drop_last=True)\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, sampler=RandomSampler(unlabeled_dataset),\n",
    "                              batch_size=args.batch_size * args.mu, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2b9bc8",
   "metadata": {},
   "source": [
    "# Supervised learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a85ca68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: 0.08M\n"
     ]
    }
   ],
   "source": [
    "teacher_model = WideResNet(num_classes=args.num_classes,\n",
    "                           depth=args.depth,\n",
    "                           widen_factor=args.widen_factor,\n",
    "                           dropout=0,\n",
    "                           dense_dropout=args.teacher_dropout)\n",
    "teacher_model.to(args.device)\n",
    "print(f\"Params: {sum(p.numel() for p in teacher_model.parameters())/1e6:.2f}M\")\n",
    "# K킬로 1000, M 메가 100만 million, G 기가 10억 billion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "846cc0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(teacher_model.parameters(), lr=args.teacher_lr, momentum=args.momentum, nesterov=args.nesterov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a339117c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Loss : 2.3356\n",
      "2 Loss : 2.1040\n",
      "3 Loss : 2.0085\n",
      "4 Loss : 1.9521\n",
      "5 Loss : 1.9039\n",
      "6 Loss : 1.8594\n",
      "7 Loss : 1.8283\n",
      "8 Loss : 1.8181\n",
      "9 Loss : 1.7788\n",
      "10 Loss : 1.7856\n",
      "11 Loss : 1.7563\n",
      "12 Loss : 1.7323\n",
      "13 Loss : 1.7137\n",
      "14 Loss : 1.7024\n",
      "15 Loss : 1.6590\n",
      "16 Loss : 1.6627\n",
      "17 Loss : 1.6569\n",
      "18 Loss : 1.6140\n",
      "19 Loss : 1.6137\n",
      "20 Loss : 1.6192\n",
      "21 Loss : 1.5772\n",
      "22 Loss : 1.5979\n",
      "23 Loss : 1.5628\n",
      "24 Loss : 1.5603\n",
      "25 Loss : 1.5550\n",
      "26 Loss : 1.5562\n",
      "27 Loss : 1.5239\n",
      "28 Loss : 1.5268\n",
      "29 Loss : 1.5118\n",
      "30 Loss : 1.4932\n",
      "31 Loss : 1.4881\n",
      "32 Loss : 1.5004\n",
      "33 Loss : 1.4744\n",
      "34 Loss : 1.4648\n",
      "35 Loss : 1.4302\n",
      "36 Loss : 1.4433\n",
      "37 Loss : 1.4216\n",
      "38 Loss : 1.4106\n",
      "39 Loss : 1.4329\n",
      "40 Loss : 1.4056\n",
      "41 Loss : 1.3836\n",
      "42 Loss : 1.3731\n",
      "43 Loss : 1.3744\n",
      "44 Loss : 1.3725\n",
      "45 Loss : 1.3454\n",
      "46 Loss : 1.3518\n",
      "47 Loss : 1.3360\n",
      "48 Loss : 1.3538\n",
      "49 Loss : 1.2863\n",
      "50 Loss : 1.2770\n",
      "51 Loss : 1.3090\n",
      "52 Loss : 1.2970\n",
      "53 Loss : 1.2912\n",
      "54 Loss : 1.2287\n",
      "55 Loss : 1.2644\n",
      "56 Loss : 1.2571\n",
      "57 Loss : 1.2468\n",
      "58 Loss : 1.2252\n",
      "59 Loss : 1.1935\n",
      "60 Loss : 1.1927\n",
      "61 Loss : 1.1988\n",
      "62 Loss : 1.2495\n",
      "63 Loss : 1.1663\n",
      "64 Loss : 1.1988\n",
      "65 Loss : 1.1678\n",
      "66 Loss : 1.1587\n",
      "67 Loss : 1.1310\n",
      "68 Loss : 1.1675\n",
      "69 Loss : 1.1420\n",
      "70 Loss : 1.1331\n",
      "71 Loss : 1.0826\n",
      "72 Loss : 1.0955\n",
      "73 Loss : 1.0973\n",
      "74 Loss : 1.0993\n",
      "75 Loss : 1.1092\n",
      "76 Loss : 1.0821\n",
      "77 Loss : 1.0946\n",
      "78 Loss : 1.0720\n",
      "79 Loss : 1.0321\n",
      "80 Loss : 1.0517\n",
      "81 Loss : 1.0214\n",
      "82 Loss : 1.0184\n",
      "83 Loss : 1.0201\n",
      "84 Loss : 1.0158\n",
      "85 Loss : 1.0317\n",
      "86 Loss : 1.0269\n",
      "87 Loss : 0.9825\n",
      "88 Loss : 0.9801\n",
      "89 Loss : 0.9997\n",
      "90 Loss : 0.9379\n",
      "91 Loss : 1.0144\n",
      "92 Loss : 0.9353\n",
      "93 Loss : 0.9751\n",
      "94 Loss : 0.9210\n",
      "95 Loss : 0.9429\n",
      "96 Loss : 0.9320\n",
      "97 Loss : 0.9455\n",
      "98 Loss : 0.9079\n",
      "99 Loss : 0.9319\n",
      "100 Loss : 0.9505\n",
      "Training complete in 0m 40s\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "\n",
    "for epoch in range(args.total_epoch):\n",
    "    # 모델은 training mode로 설정\n",
    "    teacher_model.train()\n",
    "    \n",
    "    running_loss = 0\n",
    "    running_total = 0\n",
    "    \n",
    "    for inputs, targets in labeled_loader:\n",
    "        inputs = inputs.to(args.device)\n",
    "        targets = targets.to(args.device, dtype=torch.long)\n",
    "        \n",
    "        # parameter gradients를 0으로 설정\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward\n",
    "        outputs = teacher_model(inputs)\n",
    "        #print(outputs)\n",
    "        #print(targets)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # batch별 loss를 축적함\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_total += inputs.size(0)\n",
    "\n",
    "    # epoch의 loss 도출\n",
    "    epoch_loss = running_loss / running_total\n",
    "    print(f'{epoch+1} Loss : {epoch_loss:.4f}')\n",
    "    \n",
    "time_elapsed = time.time() - since\n",
    "print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bbaa1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Acc: 0.3158\n"
     ]
    }
   ],
   "source": [
    "teacher_model.eval()\n",
    "with torch.no_grad():\n",
    "    corrects = 0\n",
    "    total = 0\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs = inputs.to(args.device)\n",
    "        targets = targets.to(args.device, dtype=torch.long)\n",
    "        \n",
    "        # forward\n",
    "        outputs = teacher_model(inputs)\n",
    "        \n",
    "        # output 중 최대값의 위치에 해당하는 class로 예측 수행\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        # batch별 정답 개수를 축적함\n",
    "        corrects += torch.sum(preds == targets.data)\n",
    "        total += targets.size(0)\n",
    "\n",
    "test_acc = corrects.double() / total\n",
    "print('Testing Acc: {:.4f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8568bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_parameter = teacher_model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655c24b1",
   "metadata": {},
   "source": [
    "# Semi-supervized learning using pseudo labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5a98fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WideResNet(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (block1): NetworkBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block2): NetworkBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (convShortcut): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (block3): NetworkBlock(\n",
       "    (layer): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "        (relu2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (convShortcut): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.001, affine=True, track_running_stats=True)\n",
       "  (relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
       "  (drop): Dropout(p=0, inplace=False)\n",
       "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_model = WideResNet(num_classes=args.num_classes,\n",
    "                           depth=args.depth,\n",
    "                           widen_factor=args.widen_factor,\n",
    "                           dropout=0,\n",
    "                           dense_dropout=args.teacher_dropout)\n",
    "teacher_model.to(args.device)\n",
    "\n",
    "student_model = WideResNet(num_classes=args.num_classes,\n",
    "                           depth=args.depth,\n",
    "                           widen_factor=args.widen_factor,\n",
    "                           dropout=0,\n",
    "                           dense_dropout=args.teacher_dropout)\n",
    "student_model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2166cf5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher_model.load_state_dict(teacher_model_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "159709b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_optimizer = optim.SGD(teacher_model.parameters(),\n",
    "                        lr=args.teacher_lr,\n",
    "                        momentum=args.momentum,\n",
    "                        nesterov=args.nesterov)\n",
    "s_optimizer = optim.SGD(student_model.parameters(),\n",
    "                        lr=args.student_lr,\n",
    "                        momentum=args.momentum,\n",
    "                        nesterov=args.nesterov)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee34ea1f",
   "metadata": {},
   "source": [
    "### 예시 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b6f782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pseudo_labeling_ex1(args, teacher_model, student_model, t_optimizer, s_optimizer, criterion):\n",
    "    since = time.time()\n",
    "    \n",
    "    labeled_iter = iter(labeled_loader)\n",
    "    unlabeled_iter = iter(unlabeled_loader)\n",
    "    for step in range(args.start_step, args.total_steps):\n",
    "        if step % args.eval_step == 0:\n",
    "            if step != 0:\n",
    "                print('{} Step - Student loss: {:.4f}\\nl_loss: {:.4f} u_loss: {:.4f}'.format(step, np.mean(s_losses),\n",
    "                                                                                             np.mean(l_losses), \n",
    "                                                                                             np.mean(u_losses)))\n",
    "                \n",
    "            s_losses = []\n",
    "            l_losses = []\n",
    "            u_losses = []\n",
    "\n",
    "        try:\n",
    "            images_l, targets = labeled_iter.next()\n",
    "        except:\n",
    "            labeled_iter = iter(labeled_loader)\n",
    "            images_l, targets = labeled_iter.next()\n",
    "\n",
    "        try:\n",
    "            (images_uw, images_us), _ = unlabeled_iter.next()\n",
    "        except:\n",
    "            unlabeled_iter = iter(unlabeled_loader)\n",
    "            (images_uw, images_us), _ = unlabeled_iter.next()\n",
    "\n",
    "        images_l = images_l.to(args.device)\n",
    "        images_uw = images_uw.to(args.device)\n",
    "        images_us = images_us.to(args.device)\n",
    "        targets = targets.to(args.device, dtype=torch.long)\n",
    "\n",
    "        # parameter gradients를 0으로 설정\n",
    "        s_optimizer.zero_grad()\n",
    "\n",
    "        # forward teacher model\n",
    "        teacher_model.eval()\n",
    "        batch_size = images_l.shape[0]\n",
    "        with torch.no_grad(): \n",
    "            t_logits_uw = teacher_model(images_uw)\n",
    "\n",
    "            # make pseudo label\n",
    "            soft_pseudo_label = torch.softmax(t_logits_uw, dim=-1)\n",
    "            max_probs, hard_pseudo_label = torch.max(soft_pseudo_label, dim=-1)\n",
    "        \n",
    "        # forward student model\n",
    "        student_model.train()\n",
    "        s_images = torch.cat((images_l, images_us))\n",
    "        s_logits = student_model(s_images)\n",
    "        s_logits_l = s_logits[:batch_size]\n",
    "        s_logits_us = s_logits[batch_size:]\n",
    "        del s_logits\n",
    "\n",
    "        s_loss_l = criterion(s_logits_l, targets)\n",
    "        s_loss_u = criterion(s_logits_us, hard_pseudo_label.detach())\n",
    "        s_loss = s_loss_l + (args.lambda_u * s_loss_u)\n",
    "\n",
    "        # backward\n",
    "        s_loss.backward()\n",
    "        s_optimizer.step()\n",
    "\n",
    "        s_losses.append(s_loss.item())\n",
    "        l_losses.append(s_loss_l.item())\n",
    "        u_losses.append(s_loss_u.item())\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68c7bbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 Step - Student loss: 4.3064\n",
      "l_loss: 2.3337 u_loss: 1.9727\n",
      "40 Step - Student loss: 3.9344\n",
      "l_loss: 2.1843 u_loss: 1.7502\n",
      "60 Step - Student loss: 3.7608\n",
      "l_loss: 2.0563 u_loss: 1.7045\n",
      "80 Step - Student loss: 3.6529\n",
      "l_loss: 1.9557 u_loss: 1.6972\n",
      "100 Step - Student loss: 3.5785\n",
      "l_loss: 1.9074 u_loss: 1.6711\n",
      "120 Step - Student loss: 3.5065\n",
      "l_loss: 1.8750 u_loss: 1.6314\n",
      "140 Step - Student loss: 3.4269\n",
      "l_loss: 1.8336 u_loss: 1.5933\n",
      "160 Step - Student loss: 3.4016\n",
      "l_loss: 1.7993 u_loss: 1.6023\n",
      "180 Step - Student loss: 3.3223\n",
      "l_loss: 1.8211 u_loss: 1.5011\n",
      "200 Step - Student loss: 3.3499\n",
      "l_loss: 1.7631 u_loss: 1.5867\n",
      "220 Step - Student loss: 3.2378\n",
      "l_loss: 1.7376 u_loss: 1.5002\n",
      "240 Step - Student loss: 3.2626\n",
      "l_loss: 1.7096 u_loss: 1.5530\n",
      "260 Step - Student loss: 3.2310\n",
      "l_loss: 1.6889 u_loss: 1.5421\n",
      "280 Step - Student loss: 3.1300\n",
      "l_loss: 1.6593 u_loss: 1.4707\n",
      "300 Step - Student loss: 3.2086\n",
      "l_loss: 1.6622 u_loss: 1.5463\n",
      "320 Step - Student loss: 3.1528\n",
      "l_loss: 1.6220 u_loss: 1.5308\n",
      "340 Step - Student loss: 3.1005\n",
      "l_loss: 1.5954 u_loss: 1.5051\n",
      "360 Step - Student loss: 3.0699\n",
      "l_loss: 1.5950 u_loss: 1.4749\n",
      "380 Step - Student loss: 3.0421\n",
      "l_loss: 1.5415 u_loss: 1.5006\n",
      "400 Step - Student loss: 3.0248\n",
      "l_loss: 1.5463 u_loss: 1.4784\n",
      "420 Step - Student loss: 3.0395\n",
      "l_loss: 1.5328 u_loss: 1.5067\n",
      "440 Step - Student loss: 2.9798\n",
      "l_loss: 1.5192 u_loss: 1.4606\n",
      "460 Step - Student loss: 2.9630\n",
      "l_loss: 1.4904 u_loss: 1.4726\n",
      "480 Step - Student loss: 2.9162\n",
      "l_loss: 1.5053 u_loss: 1.4109\n",
      "500 Step - Student loss: 2.9570\n",
      "l_loss: 1.4793 u_loss: 1.4777\n",
      "520 Step - Student loss: 2.9136\n",
      "l_loss: 1.4897 u_loss: 1.4239\n",
      "540 Step - Student loss: 2.8936\n",
      "l_loss: 1.4425 u_loss: 1.4512\n",
      "560 Step - Student loss: 2.9111\n",
      "l_loss: 1.4620 u_loss: 1.4491\n",
      "580 Step - Student loss: 2.8830\n",
      "l_loss: 1.4456 u_loss: 1.4374\n",
      "600 Step - Student loss: 2.8649\n",
      "l_loss: 1.4063 u_loss: 1.4587\n",
      "620 Step - Student loss: 2.8517\n",
      "l_loss: 1.4026 u_loss: 1.4490\n",
      "640 Step - Student loss: 2.8260\n",
      "l_loss: 1.4198 u_loss: 1.4062\n",
      "660 Step - Student loss: 2.7955\n",
      "l_loss: 1.3740 u_loss: 1.4216\n",
      "680 Step - Student loss: 2.7965\n",
      "l_loss: 1.3845 u_loss: 1.4120\n",
      "700 Step - Student loss: 2.8221\n",
      "l_loss: 1.3580 u_loss: 1.4641\n",
      "720 Step - Student loss: 2.7934\n",
      "l_loss: 1.3812 u_loss: 1.4122\n",
      "740 Step - Student loss: 2.7040\n",
      "l_loss: 1.3455 u_loss: 1.3585\n",
      "760 Step - Student loss: 2.7858\n",
      "l_loss: 1.3573 u_loss: 1.4284\n",
      "780 Step - Student loss: 2.7424\n",
      "l_loss: 1.3137 u_loss: 1.4287\n",
      "800 Step - Student loss: 2.7209\n",
      "l_loss: 1.3254 u_loss: 1.3955\n",
      "820 Step - Student loss: 2.7374\n",
      "l_loss: 1.3041 u_loss: 1.4334\n",
      "840 Step - Student loss: 2.7373\n",
      "l_loss: 1.3165 u_loss: 1.4209\n",
      "860 Step - Student loss: 2.7244\n",
      "l_loss: 1.3015 u_loss: 1.4229\n",
      "880 Step - Student loss: 2.7348\n",
      "l_loss: 1.2972 u_loss: 1.4375\n",
      "900 Step - Student loss: 2.6165\n",
      "l_loss: 1.2589 u_loss: 1.3576\n",
      "920 Step - Student loss: 2.7093\n",
      "l_loss: 1.2750 u_loss: 1.4343\n",
      "940 Step - Student loss: 2.6639\n",
      "l_loss: 1.2764 u_loss: 1.3875\n",
      "960 Step - Student loss: 2.6449\n",
      "l_loss: 1.2453 u_loss: 1.3996\n",
      "980 Step - Student loss: 2.6538\n",
      "l_loss: 1.2171 u_loss: 1.4367\n",
      "1000 Step - Student loss: 2.6496\n",
      "l_loss: 1.2508 u_loss: 1.3988\n",
      "1020 Step - Student loss: 2.6371\n",
      "l_loss: 1.2671 u_loss: 1.3701\n",
      "1040 Step - Student loss: 2.6058\n",
      "l_loss: 1.2016 u_loss: 1.4042\n",
      "1060 Step - Student loss: 2.5721\n",
      "l_loss: 1.2163 u_loss: 1.3557\n",
      "1080 Step - Student loss: 2.5696\n",
      "l_loss: 1.2091 u_loss: 1.3605\n",
      "1100 Step - Student loss: 2.5204\n",
      "l_loss: 1.1459 u_loss: 1.3744\n",
      "1120 Step - Student loss: 2.6550\n",
      "l_loss: 1.2109 u_loss: 1.4441\n",
      "1140 Step - Student loss: 2.5757\n",
      "l_loss: 1.2106 u_loss: 1.3652\n",
      "1160 Step - Student loss: 2.5917\n",
      "l_loss: 1.1768 u_loss: 1.4150\n",
      "1180 Step - Student loss: 2.5491\n",
      "l_loss: 1.1623 u_loss: 1.3868\n",
      "1200 Step - Student loss: 2.5521\n",
      "l_loss: 1.1883 u_loss: 1.3638\n",
      "1220 Step - Student loss: 2.6203\n",
      "l_loss: 1.1702 u_loss: 1.4501\n",
      "1240 Step - Student loss: 2.5185\n",
      "l_loss: 1.1468 u_loss: 1.3717\n",
      "1260 Step - Student loss: 2.5372\n",
      "l_loss: 1.1331 u_loss: 1.4041\n",
      "1280 Step - Student loss: 2.4869\n",
      "l_loss: 1.1316 u_loss: 1.3553\n",
      "1300 Step - Student loss: 2.4793\n",
      "l_loss: 1.1366 u_loss: 1.3427\n",
      "1320 Step - Student loss: 2.4368\n",
      "l_loss: 1.1061 u_loss: 1.3307\n",
      "1340 Step - Student loss: 2.4698\n",
      "l_loss: 1.1133 u_loss: 1.3564\n",
      "1360 Step - Student loss: 2.5196\n",
      "l_loss: 1.1243 u_loss: 1.3953\n",
      "1380 Step - Student loss: 2.5111\n",
      "l_loss: 1.1258 u_loss: 1.3852\n",
      "1400 Step - Student loss: 2.4138\n",
      "l_loss: 1.0397 u_loss: 1.3741\n",
      "1420 Step - Student loss: 2.5250\n",
      "l_loss: 1.1207 u_loss: 1.4043\n",
      "1440 Step - Student loss: 2.3710\n",
      "l_loss: 1.0990 u_loss: 1.2720\n",
      "1460 Step - Student loss: 2.3978\n",
      "l_loss: 1.0540 u_loss: 1.3438\n",
      "1480 Step - Student loss: 2.4133\n",
      "l_loss: 1.0291 u_loss: 1.3842\n",
      "1500 Step - Student loss: 2.4573\n",
      "l_loss: 1.0691 u_loss: 1.3881\n",
      "1520 Step - Student loss: 2.3927\n",
      "l_loss: 1.0511 u_loss: 1.3416\n",
      "1540 Step - Student loss: 2.4179\n",
      "l_loss: 1.0747 u_loss: 1.3432\n",
      "1560 Step - Student loss: 2.3621\n",
      "l_loss: 1.0218 u_loss: 1.3402\n",
      "1580 Step - Student loss: 2.3546\n",
      "l_loss: 1.0144 u_loss: 1.3402\n",
      "1600 Step - Student loss: 2.3562\n",
      "l_loss: 1.0283 u_loss: 1.3280\n",
      "1620 Step - Student loss: 2.3757\n",
      "l_loss: 1.0011 u_loss: 1.3746\n",
      "1640 Step - Student loss: 2.3172\n",
      "l_loss: 0.9924 u_loss: 1.3248\n",
      "1660 Step - Student loss: 2.3593\n",
      "l_loss: 1.0174 u_loss: 1.3419\n",
      "1680 Step - Student loss: 2.4059\n",
      "l_loss: 1.0448 u_loss: 1.3611\n",
      "1700 Step - Student loss: 2.4014\n",
      "l_loss: 1.0113 u_loss: 1.3901\n",
      "1720 Step - Student loss: 2.3107\n",
      "l_loss: 0.9857 u_loss: 1.3250\n",
      "1740 Step - Student loss: 2.3006\n",
      "l_loss: 0.9965 u_loss: 1.3041\n",
      "1760 Step - Student loss: 2.3376\n",
      "l_loss: 0.9797 u_loss: 1.3579\n",
      "1780 Step - Student loss: 2.3231\n",
      "l_loss: 0.9824 u_loss: 1.3407\n",
      "1800 Step - Student loss: 2.3223\n",
      "l_loss: 0.9696 u_loss: 1.3527\n",
      "1820 Step - Student loss: 2.2846\n",
      "l_loss: 0.9446 u_loss: 1.3400\n",
      "1840 Step - Student loss: 2.3135\n",
      "l_loss: 0.9526 u_loss: 1.3610\n",
      "1860 Step - Student loss: 2.3570\n",
      "l_loss: 0.9702 u_loss: 1.3868\n",
      "1880 Step - Student loss: 2.3225\n",
      "l_loss: 0.9521 u_loss: 1.3704\n",
      "1900 Step - Student loss: 2.2798\n",
      "l_loss: 0.9283 u_loss: 1.3515\n",
      "1920 Step - Student loss: 2.2832\n",
      "l_loss: 0.9366 u_loss: 1.3466\n",
      "1940 Step - Student loss: 2.2878\n",
      "l_loss: 0.9456 u_loss: 1.3422\n",
      "1960 Step - Student loss: 2.2958\n",
      "l_loss: 0.9380 u_loss: 1.3578\n",
      "1980 Step - Student loss: 2.2294\n",
      "l_loss: 0.9343 u_loss: 1.2951\n",
      "Training complete in 2m 47s\n"
     ]
    }
   ],
   "source": [
    "train_pseudo_labeling_ex1(args, teacher_model, student_model, t_optimizer, s_optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "760d6f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        corrects = 0\n",
    "        total = 0\n",
    "        for inputs, targets in loader:\n",
    "            inputs = inputs.to(args.device)\n",
    "            targets = targets.to(args.device, dtype=torch.long)\n",
    "\n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # output 중 최대값의 위치에 해당하는 class로 예측 수행\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            # batch별 정답 개수를 축적함\n",
    "            corrects += torch.sum(preds == targets.data)\n",
    "            total += targets.size(0)\n",
    "\n",
    "    test_acc = corrects.double() / total\n",
    "    print('Testing Acc: {:.4f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b214a97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Acc: 0.3911\n"
     ]
    }
   ],
   "source": [
    "test(args, student_model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d54aac7",
   "metadata": {},
   "source": [
    "### 예시 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3f2c0431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = 3, n=10인 unlabeled 데이터셋으로 바꿔보기\n",
    "transform_unlabeled = CustomTransform(args, n=3, m=10, mean=cifar10_mean, std=cifar10_std)\n",
    "unlabeled_dataset = CustomCIFAR10SSL(args.data_path, unlabeled_idxs, train=True, transform=transform_unlabeled)\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, sampler=RandomSampler(unlabeled_dataset),\n",
    "                              batch_size=args.batch_size * args.mu, drop_last=True)\n",
    "\n",
    "# Labeled 데이터로 Teacher 모델 사전 학습\n",
    "teacher_model = WideResNet(num_classes=args.num_classes,\n",
    "                           depth=args.depth,\n",
    "                           widen_factor=args.widen_factor,\n",
    "                           dropout=0,\n",
    "                           dense_dropout=args.teacher_dropout)\n",
    "teacher_model.to(args.device)\n",
    "teacher_model.load_state_dict(teacher_model_parameter)\n",
    "\n",
    "# Student 모델 준비\n",
    "student_model = WideResNet(num_classes=args.num_classes,\n",
    "                           depth=args.depth,\n",
    "                           widen_factor=args.widen_factor,\n",
    "                           dropout=0,\n",
    "                           dense_dropout=args.teacher_dropout)\n",
    "student_model.to(args.device)\n",
    "\n",
    "# Optimizer, criterion 준비\n",
    "t_optimizer = optim.SGD(teacher_model.parameters(),\n",
    "                        lr=args.teacher_lr,\n",
    "                        momentum=args.momentum,\n",
    "                        nesterov=args.nesterov)\n",
    "s_optimizer = optim.SGD(student_model.parameters(),\n",
    "                        lr=args.student_lr,\n",
    "                        momentum=args.momentum,\n",
    "                        nesterov=args.nesterov)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7ca6557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pseudo_labeling_ex2(args, teacher_model, student_model, t_optimizer, s_optimizer, criterion):\n",
    "    since = time.time()\n",
    "    \n",
    "    labeled_iter = iter(labeled_loader)\n",
    "    unlabeled_iter = iter(unlabeled_loader)\n",
    "    for step in range(args.start_step, args.total_steps):\n",
    "        if step % args.eval_step == 0:\n",
    "            if step != 0:\n",
    "                print('{} Step - Teacher loss: {:.4f} Student loss: {:.4f}\\nl_loss: {:.4f} u_loss: {:.4f}'.format(step, np.mean(t_losses), np.mean(s_losses),\n",
    "                                                                                    np.mean(l_losses), np.mean(u_losses)))\n",
    "                \n",
    "            s_losses = []\n",
    "            t_losses = []\n",
    "            l_losses = []\n",
    "            u_losses = []\n",
    "            \n",
    "        teacher_model.train()\n",
    "        student_model.train()\n",
    "\n",
    "        try:\n",
    "            images_l, targets = labeled_iter.next()\n",
    "        except:\n",
    "            labeled_iter = iter(labeled_loader)\n",
    "            images_l, targets = labeled_iter.next()\n",
    "\n",
    "        try:\n",
    "            (images_uw, images_us), _ = unlabeled_iter.next()\n",
    "        except:\n",
    "            unlabeled_iter = iter(unlabeled_loader)\n",
    "            (images_uw, images_us), _ = unlabeled_iter.next()\n",
    "\n",
    "        images_l = images_l.to(args.device)\n",
    "        images_uw = images_uw.to(args.device)\n",
    "        images_us = images_us.to(args.device)\n",
    "        targets = targets.to(args.device, dtype=torch.long)\n",
    "\n",
    "        # parameter gradients를 0으로 설정\n",
    "        t_optimizer.zero_grad()\n",
    "        s_optimizer.zero_grad()\n",
    "\n",
    "        # forward teacher model\n",
    "        batch_size = images_l.shape[0]\n",
    "        t_images = torch.cat((images_l, images_uw))\n",
    "        t_logits = teacher_model(t_images)\n",
    "        t_logits_l = t_logits[:batch_size]\n",
    "        t_logits_uw = t_logits[batch_size:]\n",
    "        del t_logits\n",
    "\n",
    "        t_loss_l = criterion(t_logits_l, targets)\n",
    "\n",
    "        # make pseudo label\n",
    "        soft_pseudo_label = torch.softmax(t_logits_uw, dim=-1)\n",
    "        max_probs, hard_pseudo_label = torch.max(soft_pseudo_label, dim=-1)\n",
    "        \n",
    "        # forward student model\n",
    "        s_images = torch.cat((images_l, images_us))\n",
    "        s_logits = student_model(s_images)\n",
    "        s_logits_l = s_logits[:batch_size]\n",
    "        s_logits_us = s_logits[batch_size:]\n",
    "        del s_logits\n",
    "\n",
    "        s_loss_l = criterion(s_logits_l, targets)\n",
    "        s_loss_u = criterion(s_logits_us, hard_pseudo_label.detach())\n",
    "        s_loss = s_loss_l + (args.lambda_u * s_loss_u)\n",
    "\n",
    "        # backward\n",
    "        t_loss_l.backward()\n",
    "        t_optimizer.step()\n",
    "        \n",
    "        s_loss.backward()\n",
    "        s_optimizer.step()\n",
    "\n",
    "        s_losses.append(s_loss.item())\n",
    "        t_losses.append(t_loss_l.item())\n",
    "        l_losses.append(s_loss_l.item())\n",
    "        u_losses.append(s_loss_u.item())\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c202e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 Step - Student loss: 4.2802\n",
      "l_loss: 2.3243 u_loss: 1.9560\n",
      "40 Step - Student loss: 3.9065\n",
      "l_loss: 2.1944 u_loss: 1.7121\n",
      "60 Step - Student loss: 3.7735\n",
      "l_loss: 2.0625 u_loss: 1.7110\n",
      "80 Step - Student loss: 3.6201\n",
      "l_loss: 2.0272 u_loss: 1.5929\n",
      "100 Step - Student loss: 3.5334\n",
      "l_loss: 1.9532 u_loss: 1.5802\n",
      "120 Step - Student loss: 3.5132\n",
      "l_loss: 1.9343 u_loss: 1.5788\n",
      "140 Step - Student loss: 3.3741\n",
      "l_loss: 1.8571 u_loss: 1.5170\n",
      "160 Step - Student loss: 3.3423\n",
      "l_loss: 1.8296 u_loss: 1.5127\n",
      "180 Step - Student loss: 3.2943\n",
      "l_loss: 1.7943 u_loss: 1.5000\n",
      "200 Step - Student loss: 3.2341\n",
      "l_loss: 1.7610 u_loss: 1.4731\n",
      "220 Step - Student loss: 3.2417\n",
      "l_loss: 1.7399 u_loss: 1.5018\n",
      "240 Step - Student loss: 3.2071\n",
      "l_loss: 1.7367 u_loss: 1.4704\n",
      "260 Step - Student loss: 3.2097\n",
      "l_loss: 1.7132 u_loss: 1.4966\n",
      "280 Step - Student loss: 3.1126\n",
      "l_loss: 1.6627 u_loss: 1.4499\n",
      "300 Step - Student loss: 3.1135\n",
      "l_loss: 1.7001 u_loss: 1.4134\n",
      "320 Step - Student loss: 3.0648\n",
      "l_loss: 1.6243 u_loss: 1.4405\n",
      "340 Step - Student loss: 3.0428\n",
      "l_loss: 1.6397 u_loss: 1.4031\n",
      "360 Step - Student loss: 3.0081\n",
      "l_loss: 1.6150 u_loss: 1.3932\n",
      "380 Step - Student loss: 3.0378\n",
      "l_loss: 1.6106 u_loss: 1.4272\n",
      "400 Step - Student loss: 2.9289\n",
      "l_loss: 1.5285 u_loss: 1.4005\n",
      "420 Step - Student loss: 2.9510\n",
      "l_loss: 1.5708 u_loss: 1.3802\n",
      "440 Step - Student loss: 2.9083\n",
      "l_loss: 1.5273 u_loss: 1.3810\n",
      "460 Step - Student loss: 2.9992\n",
      "l_loss: 1.5481 u_loss: 1.4511\n",
      "480 Step - Student loss: 2.8908\n",
      "l_loss: 1.5101 u_loss: 1.3807\n",
      "500 Step - Student loss: 2.8602\n",
      "l_loss: 1.5156 u_loss: 1.3446\n",
      "520 Step - Student loss: 2.8316\n",
      "l_loss: 1.4900 u_loss: 1.3417\n",
      "540 Step - Student loss: 2.8426\n",
      "l_loss: 1.4774 u_loss: 1.3652\n",
      "560 Step - Student loss: 2.8106\n",
      "l_loss: 1.4517 u_loss: 1.3589\n",
      "580 Step - Student loss: 2.7955\n",
      "l_loss: 1.4150 u_loss: 1.3805\n",
      "600 Step - Student loss: 2.7933\n",
      "l_loss: 1.4532 u_loss: 1.3401\n",
      "620 Step - Student loss: 2.7443\n",
      "l_loss: 1.4040 u_loss: 1.3403\n",
      "640 Step - Student loss: 2.7042\n",
      "l_loss: 1.3823 u_loss: 1.3219\n",
      "660 Step - Student loss: 2.6765\n",
      "l_loss: 1.3695 u_loss: 1.3070\n",
      "680 Step - Student loss: 2.6735\n",
      "l_loss: 1.3566 u_loss: 1.3169\n",
      "700 Step - Student loss: 2.6617\n",
      "l_loss: 1.3547 u_loss: 1.3070\n",
      "720 Step - Student loss: 2.6834\n",
      "l_loss: 1.3302 u_loss: 1.3532\n",
      "740 Step - Student loss: 2.6511\n",
      "l_loss: 1.3326 u_loss: 1.3186\n",
      "760 Step - Student loss: 2.6838\n",
      "l_loss: 1.3243 u_loss: 1.3595\n",
      "780 Step - Student loss: 2.5786\n",
      "l_loss: 1.3006 u_loss: 1.2780\n",
      "800 Step - Student loss: 2.6401\n",
      "l_loss: 1.3064 u_loss: 1.3337\n",
      "820 Step - Student loss: 2.5167\n",
      "l_loss: 1.2465 u_loss: 1.2702\n",
      "840 Step - Student loss: 2.6076\n",
      "l_loss: 1.2878 u_loss: 1.3198\n",
      "860 Step - Student loss: 2.6197\n",
      "l_loss: 1.2799 u_loss: 1.3398\n",
      "880 Step - Student loss: 2.5664\n",
      "l_loss: 1.2679 u_loss: 1.2985\n",
      "900 Step - Student loss: 2.6044\n",
      "l_loss: 1.2694 u_loss: 1.3350\n",
      "920 Step - Student loss: 2.5263\n",
      "l_loss: 1.2295 u_loss: 1.2968\n",
      "940 Step - Student loss: 2.5689\n",
      "l_loss: 1.2594 u_loss: 1.3096\n",
      "960 Step - Student loss: 2.5409\n",
      "l_loss: 1.2320 u_loss: 1.3089\n",
      "980 Step - Student loss: 2.4703\n",
      "l_loss: 1.2008 u_loss: 1.2695\n",
      "1000 Step - Student loss: 2.4432\n",
      "l_loss: 1.1725 u_loss: 1.2707\n",
      "1020 Step - Student loss: 2.4998\n",
      "l_loss: 1.2165 u_loss: 1.2833\n",
      "1040 Step - Student loss: 2.4594\n",
      "l_loss: 1.1537 u_loss: 1.3058\n",
      "1060 Step - Student loss: 2.4408\n",
      "l_loss: 1.1731 u_loss: 1.2677\n",
      "1080 Step - Student loss: 2.4927\n",
      "l_loss: 1.2061 u_loss: 1.2866\n",
      "1100 Step - Student loss: 2.3824\n",
      "l_loss: 1.1461 u_loss: 1.2363\n",
      "1120 Step - Student loss: 2.4403\n",
      "l_loss: 1.1319 u_loss: 1.3084\n",
      "1140 Step - Student loss: 2.3939\n",
      "l_loss: 1.1616 u_loss: 1.2323\n",
      "1160 Step - Student loss: 2.3702\n",
      "l_loss: 1.1128 u_loss: 1.2574\n",
      "1180 Step - Student loss: 2.3977\n",
      "l_loss: 1.1272 u_loss: 1.2705\n",
      "1200 Step - Student loss: 2.3992\n",
      "l_loss: 1.1449 u_loss: 1.2543\n",
      "1220 Step - Student loss: 2.3879\n",
      "l_loss: 1.1188 u_loss: 1.2691\n",
      "1240 Step - Student loss: 2.4069\n",
      "l_loss: 1.1242 u_loss: 1.2827\n",
      "1260 Step - Student loss: 2.3637\n",
      "l_loss: 1.0792 u_loss: 1.2845\n",
      "1280 Step - Student loss: 2.3312\n",
      "l_loss: 1.0833 u_loss: 1.2478\n",
      "1300 Step - Student loss: 2.4053\n",
      "l_loss: 1.1176 u_loss: 1.2877\n",
      "1320 Step - Student loss: 2.3559\n",
      "l_loss: 1.0893 u_loss: 1.2666\n",
      "1340 Step - Student loss: 2.3277\n",
      "l_loss: 1.0470 u_loss: 1.2807\n",
      "1360 Step - Student loss: 2.2930\n",
      "l_loss: 1.0558 u_loss: 1.2372\n",
      "1380 Step - Student loss: 2.3729\n",
      "l_loss: 1.0854 u_loss: 1.2875\n",
      "1400 Step - Student loss: 2.3540\n",
      "l_loss: 1.0579 u_loss: 1.2962\n",
      "1420 Step - Student loss: 2.3548\n",
      "l_loss: 1.0623 u_loss: 1.2925\n",
      "1440 Step - Student loss: 2.3061\n",
      "l_loss: 1.0313 u_loss: 1.2748\n",
      "1460 Step - Student loss: 2.3116\n",
      "l_loss: 1.0164 u_loss: 1.2952\n",
      "1480 Step - Student loss: 2.2397\n",
      "l_loss: 1.0237 u_loss: 1.2161\n",
      "1500 Step - Student loss: 2.2789\n",
      "l_loss: 1.0038 u_loss: 1.2751\n",
      "1520 Step - Student loss: 2.2609\n",
      "l_loss: 1.0258 u_loss: 1.2351\n",
      "1540 Step - Student loss: 2.2114\n",
      "l_loss: 0.9594 u_loss: 1.2519\n",
      "1560 Step - Student loss: 2.2300\n",
      "l_loss: 0.9965 u_loss: 1.2335\n",
      "1580 Step - Student loss: 2.2187\n",
      "l_loss: 0.9383 u_loss: 1.2804\n",
      "1600 Step - Student loss: 2.1765\n",
      "l_loss: 0.9837 u_loss: 1.1928\n",
      "1620 Step - Student loss: 2.1625\n",
      "l_loss: 0.9681 u_loss: 1.1944\n",
      "1640 Step - Student loss: 2.2378\n",
      "l_loss: 0.9731 u_loss: 1.2646\n",
      "1660 Step - Student loss: 2.2239\n",
      "l_loss: 0.9841 u_loss: 1.2398\n",
      "1680 Step - Student loss: 2.1851\n",
      "l_loss: 0.9447 u_loss: 1.2404\n",
      "1700 Step - Student loss: 2.1671\n",
      "l_loss: 0.9468 u_loss: 1.2203\n",
      "1720 Step - Student loss: 2.1655\n",
      "l_loss: 0.9309 u_loss: 1.2346\n",
      "1740 Step - Student loss: 2.2047\n",
      "l_loss: 0.9295 u_loss: 1.2752\n",
      "1760 Step - Student loss: 2.1828\n",
      "l_loss: 0.9190 u_loss: 1.2637\n",
      "1780 Step - Student loss: 2.2123\n",
      "l_loss: 0.9400 u_loss: 1.2723\n",
      "1800 Step - Student loss: 2.2091\n",
      "l_loss: 0.9162 u_loss: 1.2929\n",
      "1820 Step - Student loss: 2.1352\n",
      "l_loss: 0.9154 u_loss: 1.2198\n",
      "1840 Step - Student loss: 2.1738\n",
      "l_loss: 0.9026 u_loss: 1.2712\n",
      "1860 Step - Student loss: 2.1424\n",
      "l_loss: 0.9172 u_loss: 1.2252\n",
      "1880 Step - Student loss: 2.1454\n",
      "l_loss: 0.8737 u_loss: 1.2717\n",
      "1900 Step - Student loss: 2.1404\n",
      "l_loss: 0.9039 u_loss: 1.2365\n",
      "1920 Step - Student loss: 2.1437\n",
      "l_loss: 0.9022 u_loss: 1.2416\n",
      "1940 Step - Student loss: 2.1074\n",
      "l_loss: 0.9110 u_loss: 1.1963\n",
      "1960 Step - Student loss: 2.1056\n",
      "l_loss: 0.8723 u_loss: 1.2333\n",
      "1980 Step - Student loss: 2.1550\n",
      "l_loss: 0.8853 u_loss: 1.2697\n",
      "Training complete in 2m 35s\n"
     ]
    }
   ],
   "source": [
    "train_pseudo_labeling_ex1(args, teacher_model, student_model, t_optimizer, s_optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "87685c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Acc: 0.3995\n"
     ]
    }
   ],
   "source": [
    "test(args, student_model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
